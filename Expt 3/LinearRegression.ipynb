{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>X1 transaction date</th>\n",
       "      <th>X2 house age</th>\n",
       "      <th>X3 distance to the nearest MRT station</th>\n",
       "      <th>X4 number of convenience stores</th>\n",
       "      <th>X5 latitude</th>\n",
       "      <th>X6 longitude</th>\n",
       "      <th>Y house price of unit area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2012.917</td>\n",
       "      <td>32.0</td>\n",
       "      <td>84.87882</td>\n",
       "      <td>10</td>\n",
       "      <td>24.98298</td>\n",
       "      <td>121.54024</td>\n",
       "      <td>37.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2012.917</td>\n",
       "      <td>19.5</td>\n",
       "      <td>306.59470</td>\n",
       "      <td>9</td>\n",
       "      <td>24.98034</td>\n",
       "      <td>121.53951</td>\n",
       "      <td>42.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2013.583</td>\n",
       "      <td>13.3</td>\n",
       "      <td>561.98450</td>\n",
       "      <td>5</td>\n",
       "      <td>24.98746</td>\n",
       "      <td>121.54391</td>\n",
       "      <td>47.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2013.500</td>\n",
       "      <td>13.3</td>\n",
       "      <td>561.98450</td>\n",
       "      <td>5</td>\n",
       "      <td>24.98746</td>\n",
       "      <td>121.54391</td>\n",
       "      <td>54.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2012.833</td>\n",
       "      <td>5.0</td>\n",
       "      <td>390.56840</td>\n",
       "      <td>5</td>\n",
       "      <td>24.97937</td>\n",
       "      <td>121.54245</td>\n",
       "      <td>43.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   No  X1 transaction date  X2 house age  \\\n",
       "0   1             2012.917          32.0   \n",
       "1   2             2012.917          19.5   \n",
       "2   3             2013.583          13.3   \n",
       "3   4             2013.500          13.3   \n",
       "4   5             2012.833           5.0   \n",
       "\n",
       "   X3 distance to the nearest MRT station  X4 number of convenience stores  \\\n",
       "0                                84.87882                               10   \n",
       "1                               306.59470                                9   \n",
       "2                               561.98450                                5   \n",
       "3                               561.98450                                5   \n",
       "4                               390.56840                                5   \n",
       "\n",
       "   X5 latitude  X6 longitude  Y house price of unit area  \n",
       "0     24.98298     121.54024                        37.9  \n",
       "1     24.98034     121.53951                        42.2  \n",
       "2     24.98746     121.54391                        47.3  \n",
       "3     24.98746     121.54391                        54.8  \n",
       "4     24.97937     121.54245                        43.1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/Real estate.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1 transaction date</th>\n",
       "      <th>X2 house age</th>\n",
       "      <th>X3 distance to the nearest MRT station</th>\n",
       "      <th>X4 number of convenience stores</th>\n",
       "      <th>X5 latitude</th>\n",
       "      <th>X6 longitude</th>\n",
       "      <th>Y house price of unit area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012.917</td>\n",
       "      <td>32.0</td>\n",
       "      <td>84.87882</td>\n",
       "      <td>10</td>\n",
       "      <td>24.98298</td>\n",
       "      <td>121.54024</td>\n",
       "      <td>37.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012.917</td>\n",
       "      <td>19.5</td>\n",
       "      <td>306.59470</td>\n",
       "      <td>9</td>\n",
       "      <td>24.98034</td>\n",
       "      <td>121.53951</td>\n",
       "      <td>42.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013.583</td>\n",
       "      <td>13.3</td>\n",
       "      <td>561.98450</td>\n",
       "      <td>5</td>\n",
       "      <td>24.98746</td>\n",
       "      <td>121.54391</td>\n",
       "      <td>47.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013.500</td>\n",
       "      <td>13.3</td>\n",
       "      <td>561.98450</td>\n",
       "      <td>5</td>\n",
       "      <td>24.98746</td>\n",
       "      <td>121.54391</td>\n",
       "      <td>54.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012.833</td>\n",
       "      <td>5.0</td>\n",
       "      <td>390.56840</td>\n",
       "      <td>5</td>\n",
       "      <td>24.97937</td>\n",
       "      <td>121.54245</td>\n",
       "      <td>43.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X1 transaction date  X2 house age  X3 distance to the nearest MRT station  \\\n",
       "0             2012.917          32.0                                84.87882   \n",
       "1             2012.917          19.5                               306.59470   \n",
       "2             2013.583          13.3                               561.98450   \n",
       "3             2013.500          13.3                               561.98450   \n",
       "4             2012.833           5.0                               390.56840   \n",
       "\n",
       "   X4 number of convenience stores  X5 latitude  X6 longitude  \\\n",
       "0                               10     24.98298     121.54024   \n",
       "1                                9     24.98034     121.53951   \n",
       "2                                5     24.98746     121.54391   \n",
       "3                                5     24.98746     121.54391   \n",
       "4                                5     24.97937     121.54245   \n",
       "\n",
       "   Y house price of unit area  \n",
       "0                        37.9  \n",
       "1                        42.2  \n",
       "2                        47.3  \n",
       "3                        54.8  \n",
       "4                        43.1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df['No']\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      32.0\n",
       "1      19.5\n",
       "2      13.3\n",
       "3      13.3\n",
       "4       5.0\n",
       "       ... \n",
       "409    13.7\n",
       "410     5.6\n",
       "411    18.8\n",
       "412     8.1\n",
       "413     6.5\n",
       "Name: X2 house age, Length: 414, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['X2 house age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1 transaction date</th>\n",
       "      <th>X2 house age</th>\n",
       "      <th>X3 distance to the nearest MRT station</th>\n",
       "      <th>X4 number of convenience stores</th>\n",
       "      <th>X5 latitude</th>\n",
       "      <th>X6 longitude</th>\n",
       "      <th>Y house price of unit area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012.917</td>\n",
       "      <td>4</td>\n",
       "      <td>84.87882</td>\n",
       "      <td>10</td>\n",
       "      <td>24.98298</td>\n",
       "      <td>121.54024</td>\n",
       "      <td>37.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012.917</td>\n",
       "      <td>2</td>\n",
       "      <td>306.59470</td>\n",
       "      <td>9</td>\n",
       "      <td>24.98034</td>\n",
       "      <td>121.53951</td>\n",
       "      <td>42.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013.583</td>\n",
       "      <td>1</td>\n",
       "      <td>561.98450</td>\n",
       "      <td>5</td>\n",
       "      <td>24.98746</td>\n",
       "      <td>121.54391</td>\n",
       "      <td>47.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013.500</td>\n",
       "      <td>1</td>\n",
       "      <td>561.98450</td>\n",
       "      <td>5</td>\n",
       "      <td>24.98746</td>\n",
       "      <td>121.54391</td>\n",
       "      <td>54.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012.833</td>\n",
       "      <td>0</td>\n",
       "      <td>390.56840</td>\n",
       "      <td>5</td>\n",
       "      <td>24.97937</td>\n",
       "      <td>121.54245</td>\n",
       "      <td>43.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X1 transaction date X2 house age  X3 distance to the nearest MRT station  \\\n",
       "0             2012.917            4                                84.87882   \n",
       "1             2012.917            2                               306.59470   \n",
       "2             2013.583            1                               561.98450   \n",
       "3             2013.500            1                               561.98450   \n",
       "4             2012.833            0                               390.56840   \n",
       "\n",
       "   X4 number of convenience stores  X5 latitude  X6 longitude  \\\n",
       "0                               10     24.98298     121.54024   \n",
       "1                                9     24.98034     121.53951   \n",
       "2                                5     24.98746     121.54391   \n",
       "3                                5     24.98746     121.54391   \n",
       "4                                5     24.97937     121.54245   \n",
       "\n",
       "   Y house price of unit area  \n",
       "0                        37.9  \n",
       "1                        42.2  \n",
       "2                        47.3  \n",
       "3                        54.8  \n",
       "4                        43.1  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditions = [df['X2 house age'] <= 7.5, (df['X2 house age'] > 7.5) & (df['X2 house age'] <= 15), (df['X2 house age'] >\n",
    "              15) & (df['X2 house age'] <= 22.5), (df['X2 house age'] > 22.5) & (df['X2 house age'] <= 30), (df['X2 house age'] > 30)]\n",
    "choices = [0, 1, 2, 3, 4]\n",
    "df['X2 house age'] = np.select(conditions, choices, default=None)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['X1 transaction date'] = df['X1 transaction date'].round()\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1 transaction date</th>\n",
       "      <th>X2 house age</th>\n",
       "      <th>X3 distance to the nearest MRT station</th>\n",
       "      <th>X4 number of convenience stores</th>\n",
       "      <th>X5 latitude</th>\n",
       "      <th>X6 longitude</th>\n",
       "      <th>Y house price of unit area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012.917</td>\n",
       "      <td>4</td>\n",
       "      <td>84.87882</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01395</td>\n",
       "      <td>0.006879</td>\n",
       "      <td>37.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012.917</td>\n",
       "      <td>2</td>\n",
       "      <td>306.59470</td>\n",
       "      <td>9</td>\n",
       "      <td>0.01131</td>\n",
       "      <td>0.006149</td>\n",
       "      <td>42.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013.583</td>\n",
       "      <td>1</td>\n",
       "      <td>561.98450</td>\n",
       "      <td>5</td>\n",
       "      <td>0.01843</td>\n",
       "      <td>0.010549</td>\n",
       "      <td>47.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013.500</td>\n",
       "      <td>1</td>\n",
       "      <td>561.98450</td>\n",
       "      <td>5</td>\n",
       "      <td>0.01843</td>\n",
       "      <td>0.010549</td>\n",
       "      <td>54.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012.833</td>\n",
       "      <td>0</td>\n",
       "      <td>390.56840</td>\n",
       "      <td>5</td>\n",
       "      <td>0.01034</td>\n",
       "      <td>0.009089</td>\n",
       "      <td>43.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X1 transaction date X2 house age  X3 distance to the nearest MRT station  \\\n",
       "0             2012.917            4                                84.87882   \n",
       "1             2012.917            2                               306.59470   \n",
       "2             2013.583            1                               561.98450   \n",
       "3             2013.500            1                               561.98450   \n",
       "4             2012.833            0                               390.56840   \n",
       "\n",
       "   X4 number of convenience stores  X5 latitude  X6 longitude  \\\n",
       "0                               10      0.01395      0.006879   \n",
       "1                                9      0.01131      0.006149   \n",
       "2                                5      0.01843      0.010549   \n",
       "3                                5      0.01843      0.010549   \n",
       "4                                5      0.01034      0.009089   \n",
       "\n",
       "   Y house price of unit area  \n",
       "0                        37.9  \n",
       "1                        42.2  \n",
       "2                        47.3  \n",
       "3                        54.8  \n",
       "4                        43.1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['X5 latitude'] = df['X5 latitude'] - df['X5 latitude'].mean()\n",
    "df['X6 longitude'] = df['X6 longitude'] - df['X6 longitude'].mean()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(actual_values: np.array, training_data: pd.DataFrame, coefs: list, constant: int) -> float:\n",
    "\n",
    "    if(len(actual_values) != len(training_data.index)):\n",
    "        raise ValueError(\n",
    "            'no. of actual values should be the same as number of feature vectors')\n",
    "\n",
    "    if(training_data.shape[1] != len(coefs)):\n",
    "        raise ValueError(\n",
    "            'Number of coefs should match number of independent variables')\n",
    "\n",
    "    # print(f'coefs: {coefs}')\n",
    "\n",
    "    loss = 0\n",
    "    n = len(actual_values)\n",
    "\n",
    "    for index, row in training_data.iterrows():\n",
    "\n",
    "        r = row.to_numpy()\n",
    "        t = actual_values[index]\n",
    "\n",
    "        for i in range(r.size):\n",
    "            t = (t - (coefs[i] * r[i]))\n",
    "\n",
    "        t -= constant\n",
    "        loss += t*t\n",
    "\n",
    "    loss /= n\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1 transaction date</th>\n",
       "      <th>X2 house age</th>\n",
       "      <th>X3 distance to the nearest MRT station</th>\n",
       "      <th>X4 number of convenience stores</th>\n",
       "      <th>X5 latitude</th>\n",
       "      <th>X6 longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012.917</td>\n",
       "      <td>4</td>\n",
       "      <td>84.87882</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01395</td>\n",
       "      <td>0.006879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012.917</td>\n",
       "      <td>2</td>\n",
       "      <td>306.59470</td>\n",
       "      <td>9</td>\n",
       "      <td>0.01131</td>\n",
       "      <td>0.006149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013.583</td>\n",
       "      <td>1</td>\n",
       "      <td>561.98450</td>\n",
       "      <td>5</td>\n",
       "      <td>0.01843</td>\n",
       "      <td>0.010549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013.500</td>\n",
       "      <td>1</td>\n",
       "      <td>561.98450</td>\n",
       "      <td>5</td>\n",
       "      <td>0.01843</td>\n",
       "      <td>0.010549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012.833</td>\n",
       "      <td>0</td>\n",
       "      <td>390.56840</td>\n",
       "      <td>5</td>\n",
       "      <td>0.01034</td>\n",
       "      <td>0.009089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X1 transaction date X2 house age  X3 distance to the nearest MRT station  \\\n",
       "0             2012.917            4                                84.87882   \n",
       "1             2012.917            2                               306.59470   \n",
       "2             2013.583            1                               561.98450   \n",
       "3             2013.500            1                               561.98450   \n",
       "4             2012.833            0                               390.56840   \n",
       "\n",
       "   X4 number of convenience stores  X5 latitude  X6 longitude  \n",
       "0                               10      0.01395      0.006879  \n",
       "1                                9      0.01131      0.006149  \n",
       "2                                5      0.01843      0.010549  \n",
       "3                                5      0.01843      0.010549  \n",
       "4                                5      0.01034      0.009089  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[:,:-1]\n",
    "y = df['Y house price of unit area'].to_numpy()\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1 transaction date</th>\n",
       "      <th>X2 house age</th>\n",
       "      <th>X3 distance to the nearest MRT station</th>\n",
       "      <th>X4 number of convenience stores</th>\n",
       "      <th>X5 latitude</th>\n",
       "      <th>X6 longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>4</td>\n",
       "      <td>84.87882</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01395</td>\n",
       "      <td>0.006879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>2</td>\n",
       "      <td>306.59470</td>\n",
       "      <td>9</td>\n",
       "      <td>0.01131</td>\n",
       "      <td>0.006149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014.0</td>\n",
       "      <td>1</td>\n",
       "      <td>561.98450</td>\n",
       "      <td>5</td>\n",
       "      <td>0.01843</td>\n",
       "      <td>0.010549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014.0</td>\n",
       "      <td>1</td>\n",
       "      <td>561.98450</td>\n",
       "      <td>5</td>\n",
       "      <td>0.01843</td>\n",
       "      <td>0.010549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>0</td>\n",
       "      <td>390.56840</td>\n",
       "      <td>5</td>\n",
       "      <td>0.01034</td>\n",
       "      <td>0.009089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4082.01500</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.02748</td>\n",
       "      <td>-0.029551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>0</td>\n",
       "      <td>90.45606</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00530</td>\n",
       "      <td>0.009739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>2</td>\n",
       "      <td>390.96960</td>\n",
       "      <td>7</td>\n",
       "      <td>0.01020</td>\n",
       "      <td>0.006499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>1</td>\n",
       "      <td>104.81010</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.00229</td>\n",
       "      <td>0.007309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>2014.0</td>\n",
       "      <td>0</td>\n",
       "      <td>90.45606</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00530</td>\n",
       "      <td>0.009739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>414 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     X1 transaction date X2 house age  X3 distance to the nearest MRT station  \\\n",
       "0                 2013.0            4                                84.87882   \n",
       "1                 2013.0            2                               306.59470   \n",
       "2                 2014.0            1                               561.98450   \n",
       "3                 2014.0            1                               561.98450   \n",
       "4                 2013.0            0                               390.56840   \n",
       "..                   ...          ...                                     ...   \n",
       "409               2013.0            1                              4082.01500   \n",
       "410               2013.0            0                                90.45606   \n",
       "411               2013.0            2                               390.96960   \n",
       "412               2013.0            1                               104.81010   \n",
       "413               2014.0            0                                90.45606   \n",
       "\n",
       "     X4 number of convenience stores  X5 latitude  X6 longitude  \n",
       "0                                 10      0.01395      0.006879  \n",
       "1                                  9      0.01131      0.006149  \n",
       "2                                  5      0.01843      0.010549  \n",
       "3                                  5      0.01843      0.010549  \n",
       "4                                  5      0.01034      0.009089  \n",
       "..                               ...          ...           ...  \n",
       "409                                0     -0.02748     -0.029551  \n",
       "410                                9      0.00530      0.009739  \n",
       "411                                7      0.01020      0.006499  \n",
       "412                                5     -0.00229      0.007309  \n",
       "413                                9      0.00530      0.009739  \n",
       "\n",
       "[414 rows x 6 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['X1 transaction date'] = X['X1 transaction date'].round()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"X3 distance to the nearest MRT station\"] = np.log(df[\"X3 distance to the nearest MRT station\"])\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = [1,1,1,1,1,1]\n",
    "constant = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 11002425.206856698\n"
     ]
    }
   ],
   "source": [
    "loss = loss_function(y, X, coefs=coefs, constant=constant)\n",
    "print(f'loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_derivative(actual_values: np.array, training_data: pd.DataFrame, coefs: list, constant: float, wrt: int) -> float:\n",
    "\n",
    "    if(len(actual_values) != len(training_data.index)):\n",
    "        raise ValueError(\n",
    "            'no. of actual values should be the same as number of feature vectors')\n",
    "\n",
    "    if(training_data.shape[1] != len(coefs)):\n",
    "        raise ValueError(\n",
    "            'Number of coefs should match number of independent variables')\n",
    "\n",
    "    # print(f'coefs are: {coefs}')\n",
    "\n",
    "    d = 0\n",
    "    for_constant = wrt == -1\n",
    "\n",
    "    def multiplier(\n",
    "        arr: np.array) -> float: return 1 if for_constant else float(arr[wrt])\n",
    "\n",
    "    for index, row in training_data.iterrows():\n",
    "\n",
    "        r = row.to_numpy()\n",
    "        t = actual_values[index]\n",
    "\n",
    "        for i in range(r.size):\n",
    "            t = (t - (coefs[i]*r[i]))\n",
    "\n",
    "        t = t*multiplier(r)\n",
    "\n",
    "        d += t\n",
    "\n",
    "    n = len(actual_values)\n",
    "    d = d * -2/n\n",
    "\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(actual_values: np.array, training_data: pd.DataFrame, learning_rate: float = 1, learning_limiter: float = 1, minimum_loss_difference=0.00001) -> tuple:\n",
    "\n",
    "    if(len(actual_values) != len(training_data.index)):\n",
    "        raise ValueError(\n",
    "            'no. of actual values should be the same as number of feature vectors')\n",
    "\n",
    "    cols = training_data.shape[1]\n",
    "    coefs = [0 for i in range(cols)]\n",
    "    constant = 0\n",
    "\n",
    "    print(f'coefs are: {coefs}')\n",
    "    print(f'constant ->{constant}')\n",
    "    print(f'learning limiter: {learning_limiter}')\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    loss_vals = []\n",
    "\n",
    "    prev = 0\n",
    "    loss = 0\n",
    "\n",
    "    while True:\n",
    "\n",
    "        print(f'at count {count} -> {coefs} and {constant}')\n",
    "\n",
    "        for i in range(len(coefs)):\n",
    "\n",
    "            pd = partial_derivative(\n",
    "                actual_values, training_data, coefs, constant, i)\n",
    "\n",
    "            coefs[i] = coefs[i] - (learning_rate * pd)\n",
    "\n",
    "        c_pd = partial_derivative(\n",
    "            actual_values, training_data, coefs, constant, -1)\n",
    "\n",
    "        constant = constant - (learning_rate * c_pd)\n",
    "\n",
    "        loss = loss_function(actual_values, training_data,\n",
    "                             coefs, constant)\n",
    "\n",
    "        loss_vals.append(loss)\n",
    "\n",
    "        print(f'loss is {loss}')\n",
    "\n",
    "        if loss <= learning_limiter:\n",
    "            break\n",
    "\n",
    "        print(f' loss is greater than learning limiter')\n",
    "        print(f' prev is {prev}')\n",
    "        print(f' diff : {abs(prev - loss)}')\n",
    "\n",
    "        if count > 0 and abs(loss - prev) < minimum_loss_difference:\n",
    "            break\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        prev = loss\n",
    "\n",
    "    plt.plot(loss_vals)\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('Gradient descent')\n",
    "    plt.show()\n",
    "\n",
    "    return (coefs, constant)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c, i = gradient_descent(y, X, 0.00000001, 50)\n",
    "# print(f'coefs: {c}')\n",
    "# print(f'intercept: {i}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  5,  6,  9, 12, 16])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,3,4,5,6,7,9,12,14,16])\n",
    "b = np.array([3,4,7,14])\n",
    "\n",
    "c = a[np.invert(np.in1d(a,b))]\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Linear_Regression:\n",
    "\n",
    "    def __init__(self, actual_values: np.array = np.array([]), training_data: pd.DataFrame = pd.DataFrame.from_dict({}), learning_rate: float = 1, learning_limiter: float = 1):\n",
    "        self.dependent_variable = actual_values\n",
    "        self.training_data = training_data\n",
    "        self.learning_rate = learning_rate\n",
    "        self.learning_limiter = learning_limiter\n",
    "\n",
    "    def fit(self, actual_values: np.array, training_data: pd.DataFrame):\n",
    "        self.dependent_variable = actual_values\n",
    "        self.training_data = training_data\n",
    "\n",
    "    def train(self):\n",
    "        self.coefs, self.intercept = gradient_descent(\n",
    "            actual_values=self.dependent_variable, training_data=self.training_data, learning_rate=self.learning_rate, learning_limiter=self.learning_limiter)\n",
    "        \n",
    "    def sme(self):\n",
    "        return loss_function(self.dependent_variable,self.training_data,self.coefs,self.intercept)\n",
    "    \n",
    "    def rsme(self):\n",
    "        return np.sqrt(self.sme())\n",
    "\n",
    "    def predict(self, current_feature_vector: np.array):\n",
    "        return np.sum(np.multiply(current_feature_vector, self.coefs)) + self.intercept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefs are: [0, 0, 0, 0, 0, 0]\n",
      "constant ->0\n",
      "learning limiter: 50\n",
      "at count 0 -> [0, 0, 0, 0, 0, 0] and 0\n",
      "loss is 1375.9303292104667\n",
      " loss is greater than learning limiter\n",
      " prev is 0\n",
      " diff : 1375.9303292104667\n",
      "at count 1 -> [0.0015292131352657, 1.19878015000719e-06, 0.0005257865627750534, 3.291318900578006e-06, 1.9376479371270455e-09, 2.34404728353157e-09] and 6.866344083465928e-07\n",
      "loss is 1174.0769294370086\n",
      " loss is greater than learning limiter\n",
      " prev is 1375.9303292104667\n",
      " diff : 201.85339977345802\n",
      "at count 2 -> [0.002911525621720669, 2.2730046538546037e-06, 0.0009621812590223764, 6.335539696131733e-06, 3.955909906682093e-09, 4.8240965973715104e-09] and 1.3081519133132757e-06\n",
      "loss is 1011.4268102698346\n",
      " loss is greater than learning limiter\n",
      " prev is 1174.0769294370086\n",
      " diff : 162.650119167174\n",
      "at count 3 -> [0.004162746111187028, 3.2361382299975646e-06, 0.0013198466445854397, 9.157741426861269e-06, 6.040243055798518e-09, 7.415613460638198e-09] and 1.8715374157426756e-06\n",
      "loss is 879.913461615161\n",
      " loss is greater than learning limiter\n",
      " prev is 1011.4268102698346\n",
      " diff : 131.51334865467356\n",
      "at count 4 -> [0.005296936508889098, 4.10015638536428e-06, 0.001608242177554531, 1.178026827343992e-05, 8.177851870931123e-09, 1.0097011230562636e-08] and 2.3830045749247247e-06\n",
      "loss is 773.154466186412\n",
      " loss is greater than learning limiter\n",
      " prev is 879.913461615161\n",
      " diff : 106.75899542874902\n",
      "at count 5 -> [0.006326606066132444, 4.875710925476169e-06, 0.0018357584843824856, 1.4223032601543777e-05, 1.0357490744793795e-08, 1.2849318024377096e-08] and 2.8480814937681848e-06\n",
      "loss is 686.0985032265785\n",
      " loss is greater than learning limiter\n",
      " prev is 773.154466186412\n",
      " diff : 87.05596295983355\n",
      "at count 6 -> [0.007262883880654254, 5.5722770511927405e-06, 0.002009836675738513, 1.6503784312995556e-05, 1.2569288583148036e-08, 1.5655880823381974e-08] and 3.2716868720023172e-06\n",
      "loss is 614.7464417129247\n",
      " loss is greater than learning limiter\n",
      " prev is 686.0985032265785\n",
      " diff : 71.35206151365378\n",
      "at count 7 -> [0.00811567220732675, 6.198284091043547e-06, 0.002137074375999625, 1.8638350252293833e-05, 1.4804592997204846e-08, 1.8502102618786594e-08] and 3.658197688069811e-06\n",
      "loss is 555.930963424422\n",
      " loss is greater than learning limiter\n",
      " prev is 614.7464417129247\n",
      " diff : 58.81547828850273\n",
      "at count 8 -> [0.008893782714523818, 6.7612316900998696e-06, 0.00222331994511246, 2.0640847000144293e-05, 1.7055831900552e-08, 2.1375208919734518e-08] and 4.011509352341355e-06\n",
      "loss is 507.1424249756737\n",
      " loss is greater than learning limiter\n",
      " prev is 555.930963424422\n",
      " diff : 48.788538448748284\n",
      "at count 9 -> [0.009605057583842998, 7.267793073702166e-06, 0.0022737562069973715, 2.2523870014876603e-05, 1.9316390572270422e-08, 2.4264040353456705e-08] and 4.335089169386218e-06\n",
      "loss is 466.39124983232597\n",
      " loss is greater than learning limiter\n",
      " prev is 507.1424249756737\n",
      " diff : 40.75117514334772\n",
      "at count 10 -> [0.010256477139699622, 7.723906824272502e-06, 0.00229297485241853, 2.429866175315287e-05, 2.1580502463640818e-08, 2.7158868451453846e-08] and 4.632023853810125e-06\n",
      "loss is 432.0991807488088\n",
      " loss is greater than learning limiter\n",
      " prev is 466.39124983232597\n",
      " diff : 34.29206908351716\n",
      "at count 11 -> [0.01085425550763424, 8.134858449394671e-06, 0.0022850425542705185, 2.597526110856075e-05, 2.3843152217570106e-08, 3.005123203906372e-08] and 4.905061761322703e-06\n",
      "loss is 403.0133340056762\n",
      " loss is greater than learning limiter\n",
      " prev is 432.0991807488088\n",
      " diff : 29.085846743132606\n",
      "at count 12 -> [0.011403925633383875, 8.505352877108438e-06, 0.002253559717722426, 2.7562636246455655e-05, 2.609998954025939e-08, 3.293379193322794e-08] and 5.156650423067246e-06\n",
      "loss is 378.13826931015865\n",
      " loss is greater than learning limiter\n",
      " prev is 403.0133340056762\n",
      " diff : 24.87506469551755\n",
      "at count 13 -> [0.011910414846537584, 8.83957888795639e-06, 0.00220171268500519, 2.9068802682146377e-05, 2.8347252716064716e-08, 3.5800201908740896e-08] and 5.388969905809705e-06\n",
      "loss is 356.6822943772664\n",
      " loss is greater than learning limiter\n",
      " prev is 378.13826931015865\n",
      " diff : 21.455974932892275\n",
      "at count 14 -> [0.012378112020862104, 9.141266380980707e-06, 0.0021323201233964537, 3.050092824398418e-05, 3.058170069107933e-08, 3.864499412030151e-08] and 5.603962462429681e-06\n",
      "loss is 338.0150172024756\n",
      " loss is greater than learning limiter\n",
      " prev is 356.6822943772664\n",
      " diff : 18.667277174790797\n",
      "at count 15 -> [0.01281092726630823, 9.413737271027579e-06, 0.0020478742438782313, 3.186542638025303e-05, 3.280055277056693e-08, 4.1463477369460044e-08] and 5.803358885473616e-06\n",
      "loss is 321.63378623864827\n",
      " loss is greater than learning limiter\n",
      " prev is 338.0150172024756\n",
      " diff : 16.381230963827306\n",
      "at count 16 -> [0.013212344983661754, 9.65995072598763e-06, 0.0019505774258864299, 3.316803910642305e-05, 3.500143508166582e-08, 4.425164678486896e-08] and 5.988701930599022e-06\n",
      "loss is 307.13715414417516\n",
      " loss is greater than learning limiter\n",
      " prev is 321.63378623864827\n",
      " diff : 14.496632094473114\n",
      "at count 17 -> [0.01358547102033439, 9.882543373745312e-06, 0.0018423747595336683, 3.4413910745058203e-05, 3.7182333047245716e-08, 4.700610364360767e-08] and 6.16136713591859e-06\n",
      "loss is 294.20389215671025\n",
      " loss is greater than learning limiter\n",
      " prev is 307.13715414417516\n",
      " diff : 12.933261987464903\n",
      "at count 18 -> [0.013933074583611775, 1.0083865038529958e-05, 0.001724982959775558, 3.5607653482453005e-05, 3.934154920074776e-08, 4.972398420297662e-08] and 6.322581326975499e-06\n",
      "loss is 282.57639133135564\n",
      " loss is greater than learning limiter\n",
      " prev is 294.20389215671025\n",
      " diff : 11.627500825354616\n",
      "at count 19 -> [0.014257625494642124, 1.0266010504079497e-05, 0.0015999160564126097, 3.6753405652124134e-05, 4.147766574644493e-08, 5.240289653802038e-08] and 6.473439064840701e-06\n",
      "loss is 272.04753113338614\n",
      " loss is greater than learning limiter\n",
      " prev is 282.57639133135564\n",
      " diff : 10.528860197969493\n",
      "at count 20 -> [0.014561327301542906, 1.043084774567701e-05, 0.0014685082188704403, 3.7854883554014614e-05, 4.358951133586315e-08, 5.5040864491896126e-08] and 6.614917266170231e-06\n",
      "loss is 262.4502888346094\n",
      " loss is greater than learning limiter\n",
      " prev is 272.04753113338614\n",
      " diff : 9.597242298776735\n",
      "at count 21 -> [0.014846146712319515, 1.0580043023928865e-05, 0.0013319340347536042, 3.891542752826874e-05, 4.5676131590028e-08, 5.763627794561156e-08] and 6.747888198596284e-06\n",
      "loss is 253.64951560405325\n",
      " loss is greater than learning limiter\n",
      " prev is 262.4502888346094\n",
      " diff : 8.80077323055616\n",
      "at count 22 -> [0.015113839757025542, 1.0715083189435912e-05, 0.0011912265256665356, 3.993804292245154e-05, 4.773676294956821e-08, 6.018784870200121e-08] and 6.873131032195268e-06\n",
      "loss is 245.5354256187266\n",
      " loss is greater than learning limiter\n",
      " prev is 253.64951560405325\n",
      " diff : 8.114089985326643\n",
      "at count 23 -> [0.015365975043034665, 1.0837295508656826e-05, 0.0010472931522440337, 4.0925436520002876e-05, 4.9770809481245685e-08, 6.269457135732205e-08] and 6.991342107663786e-06\n",
      "loss is 238.01843967150919\n",
      " loss is greater than learning limiter\n",
      " prev is 245.5354256187266\n",
      " diff : 7.516985947217421\n",
      "at count 24 -> [0.015603954426804487, 1.0947865286733741e-05, 0.0009009300322938127, 4.188004893454235e-05, 5.177782231084075e-08, 6.515568860362396e-08] and 7.103144063959374e-06\n",
      "loss is 231.0250999311078\n",
      " loss is greater than learning limiter\n",
      " prev is 238.01843967150919\n",
      " diff : 6.993339740401382\n",
      "at count 25 -> [0.015829031389528625, 1.1047851532363942e-05, 0.0007528345710341393, 4.2804083418497545e-05, 5.3757481389077034e-08, 6.757066046705909e-08] and 7.209093952277733e-06\n",
      "loss is 224.4948319081714\n",
      " loss is greater than learning limiter\n",
      " prev is 231.0250999311078\n",
      " diff : 6.530268022936411\n",
      "at count 26 -> [0.01604232737209373, 1.1138200882529623e-05, 0.0006036166802632463, 4.369953148463157e-05, 5.57095793299357e-08, 6.993913704240168e-08] and 7.309690449120796e-06\n",
      "loss is 218.3773766156085\n",
      " loss is greater than learning limiter\n",
      " prev is 224.4948319081714\n",
      " diff : 6.11745529256288\n",
      "at count 27 -> [0.016244846296337098, 1.1219759980660924e-05, 0.0004538087436154294, 4.456819569470204e-05, 5.763400708973774e-08, 7.226093433302348e-08] and 7.40538026866357e-06\n",
      "loss is 212.63075299732228\n",
      " loss is greater than learning limiter\n",
      " prev is 218.3773766156085\n",
      " diff : 5.746623618286236\n",
      "at count 28 -> [0.016437487474342302, 1.1293286480267651e-05, 0.000303874467567267, 4.5411709930073884e-05, 5.9530741281171924e-08, 7.453601284909316e-08] and 7.496563863477432e-06\n",
      "loss is 207.21964000560834\n",
      " loss is greater than learning limiter\n",
      " prev is 212.63075299732228\n",
      " diff : 5.411112991713935\n",
      "at count 29 -> [0.016621057085062684, 1.1359458826932256e-05, 0.00015421674231246425, 4.6231557424084327e-05, 6.139983293937267e-08, 7.676445865544708e-08] and 7.583600492758125e-06\n",
      "loss is 202.11409086669934\n",
      " loss is greater than learning limiter\n",
      " prev is 207.21964000560834\n",
      " diff : 5.105549138908998\n",
      "at count 30 -> [0.016796278377613008, 1.1418884954544066e-05, 5.184622808847826e-06, 4.702908680483129e-05, 6.324139757752729e-08, 7.894646659495203e-08] and 7.666812728399954e-06\n",
      "loss is 197.28851037668628\n",
      " loss is greater than learning limiter\n",
      " prev is 202.11409086669934\n",
      " diff : 4.825580490013067\n",
      "at count 31 -> [0.016963800742839943, 1.1472110016535275e-05, -0.0001429204719765365, 4.780552636939344e-05, 6.505560638759934e-08, 8.108232544372858e-08] and 7.746490461430961e-06\n",
      "loss is 192.72084053559456\n",
      " loss is greater than learning limiter\n",
      " prev is 197.28851037668628\n",
      " diff : 4.567669841091714\n",
      "at count 32 -> [0.01712420777902535, 1.1519623259442257e-05, -0.0002898392545094865, 4.856199678590486e-05, 6.684267845784649e-08, 8.317240478175094e-08] and 7.82289446436809e-06\n",
      "loss is 188.39191126102122\n",
      " loss is greater than learning limiter\n",
      " prev is 192.72084053559456\n",
      " diff : 4.3289292745733405\n",
      "at count 33 -> [0.01727802446357312, 1.1561864134174046e-05, -0.0004353496814000862, 4.9299522398058016e-05, 6.860287389311174e-08, 8.521714338646478e-08] and 7.896259558869588e-06\n",
      "loss is 184.284921960624\n",
      " loss is greater than learning limiter\n",
      " prev is 188.39191126102122\n",
      " diff : 4.106989300397231\n",
      "at count 34 -> [0.01742572353008526, 1.159922772975699e-05, -0.0005792626145835221, 5.001904128719059e-05, 7.033648773657549e-08, 8.721703897850456e-08] and 7.96679743256806e-06\n",
      "loss is 180.3850268874736\n",
      " loss is greater than learning limiter\n",
      " prev is 184.284921960624\n",
      " diff : 3.8998950731503896\n",
      "at count 35 -> [0.017567731139172746, 1.1632069604892713e-05, -0.0007214179698289991, 5.0721414229854144e-05, 7.204384460295012e-08, 8.917263916764501e-08] and 8.034699144085066e-06\n",
      "loss is 176.67900285094439\n",
      " loss is greater than learning limiter\n",
      " prev is 180.3850268874736\n",
      " diff : 3.7060240365292145\n",
      "at count 36 -> [0.017704431921517293, 1.1660710084284241e-05, -0.0008616812982418611, 5.1407432673425534e-05, 7.372529394313481e-08, 9.108453346405413e-08] and 8.100137350888965e-06\n",
      "loss is 173.15498232216441\n",
      " loss is greater than learning limiter\n",
      " prev is 176.67900285094439\n",
      " diff : 3.524020528779971\n",
      "at count 37 -> [0.017836173462964584, 1.16854380792356e-05, -0.0009999407524729287, 5.207782583869117e-05, 7.538120586926851e-08, 9.29533462349619e-08] and 8.163268290801374e-06\n",
      "loss is 169.8022385049209\n",
      " loss is greater than learning limiter\n",
      " prev is 173.15498232216441\n",
      " diff : 3.352743817243521\n",
      "at count 38 -> [0.017963270293665904, 1.1706514485409624e-05, -0.0011361043947247758, 5.2733267046220306e-05, 7.701196747704436e-08, 9.477973050023115e-08] and 8.224233544530382e-06\n",
      "loss is 166.61101173624175\n",
      " loss is greater than learning limiter\n",
      " prev is 169.8022385049209\n",
      " diff : 3.191226768679144\n",
      "at count 39 -> [0.018086007436385557, 1.1724175204744903e-05, -0.0012700978084221044, 5.337437935257749e-05, 7.861797960919357e-08, 9.65643624722005e-08] and 8.28316160356281e-06\n",
      "loss is 163.57236879036338\n",
      " loss is greater than learning limiter\n",
      " prev is 166.61101173624175\n",
      " diff : 3.0386429458783653\n",
      "at count 40 -> [0.018204643562959533, 1.1738633833303648e-05, -0.0014018619796591226, 5.400174057285589e-05, 8.019965401030555e-08, 9.830793675572885e-08] and 8.340169265040882e-06\n",
      "loss is 160.67808840778432\n",
      " loss is greater than learning limiter\n",
      " prev is 163.57236879036338\n",
      " diff : 2.894280382579069\n",
      "at count 41 -> [0.018319413802441332, 1.1750084052175057e-05, -0.0015313514183100429, 5.461588775750983e-05, 8.17574108287144e-08, 1.0001116213375597e-07] and 8.39536287284296e-06\n",
      "loss is 157.9205677542157\n",
      " loss is greater than learning limiter\n",
      " prev is 160.67808840778432\n",
      " diff : 2.757520653568605\n",
      "at count 42 -> [0.018430532239627755, 1.175870175442862e-05, -0.0016585324920420936, 5.521732118390714e-05, 8.329167642612448e-08, 1.0167475787203264e-07] and 8.44883942195e-06\n",
      "loss is 155.29274560886907\n",
      " loss is greater than learning limiter\n",
      " prev is 157.9205677542157\n",
      " diff : 2.6278221453466415\n",
      "at count 43 -> [0.018538194138353187, 1.1764646937441317e-05, -0.0017833819494505036, 5.580650791630546e-05, 8.480288146004079e-08, 1.0329945048408455e-07] and 8.50068754127923e-06\n",
      "loss is 152.7880389480269\n",
      " loss is greater than learning limiter\n",
      " prev is 155.29274560886907\n",
      " diff : 2.5047066608421744\n",
      "at count 44 -> [0.018642577920115665, 1.1768065386660538e-05, -0.0019058856111833055, 5.638388498198714e-05, 8.629145920797336e-08, 1.0488597090405955e-07] and 8.550988368477922e-06\n",
      "loss is 150.40029027592607\n",
      " loss is greater than learning limiter\n",
      " prev is 152.7880389480269\n",
      " diff : 2.3877486721008268\n",
      "at count 45 -> [0.018743846925198312, 1.176909017396546e-05, -0.002026037210277434, 5.69498622059822e-05, 8.775784410585347e-08, 1.0643505202095995e-07] and 8.599816328669291e-06\n",
      "loss is 148.12372359838668\n",
      " loss is greater than learning limiter\n",
      " prev is 150.40029027592607\n",
      " diff : 2.2765666775393925\n",
      "at count 46 -> [0.018842150980428487, 1.1767842991213102e-05, -0.0021438373650192505, 5.750482474209373e-05, 8.920247047618198e-08, 1.0794742653296137e-07] and 8.647239827808836e-06\n",
      "loss is 145.95290736512487\n",
      " loss is greater than learning limiter\n",
      " prev is 148.12372359838668\n",
      " diff : 2.170816233261803\n",
      "at count 47 -> [0.018937627795031702, 1.1764435337265475e-05, -0.0022592926695014877, 5.8049135333749645e-05, 9.062577142416918e-08, 1.0942382508514054e-07] and 8.693321870123993e-06\n",
      "loss is 143.8827230478772\n",
      " loss is greater than learning limiter\n",
      " prev is 145.95290736512487\n",
      " diff : 2.07018431724768\n",
      "at count 48 -> [0.019030404203651086, 1.1758969574759172e-05, -0.0023724148887005465, 5.8583136334480494e-05, 9.202817788255903e-08, 1.1086497465804082e-07] and 8.738120608056543e-06\n",
      "loss is 141.90833829199397\n",
      " loss is greater than learning limiter\n",
      " prev is 143.8827230478772\n",
      " diff : 1.9743847558832215\n",
      "at count 49 -> [0.019120597273482233, 1.1751539871070224e-05, -0.002483220246366169, 5.9107151514512784e-05, 9.341011778799401e-08, 1.1227159717815215e-07] and 8.781689832190915e-06\n",
      "loss is 140.02518379371716\n",
      " loss is greater than learning limiter\n",
      " prev is 141.90833829199397\n",
      " diff : 1.8831544982768094\n",
      "at count 50 -> [0.019208315290588446, 1.1742233036319615e-05, -0.0025917287953201628, 5.962148767702579e-05, 9.477201537369762e-08, 1.1364440832462462e-07] and 8.82407940781945e-06\n",
      "loss is 138.22893322565525\n",
      " loss is greater than learning limiter\n",
      " prev is 140.02518379371716\n",
      " diff : 1.7962505680619074\n",
      "at count 51 -> [0.01929365863878623, 1.1731129269836433e-05, -0.0026979638609202967, 6.0126436105004905e-05, 9.611429056496e-08, 1.1498411650941527e-07] and 8.865335664056201e-06\n",
      "loss is 136.5154856681737\n",
      " loss is greater than learning limiter\n",
      " prev is 138.22893322565525\n",
      " diff : 1.713447557481544\n",
      "at count 52 -> [0.019376720583002095, 1.1718302825225984e-05, -0.002801951549475861, 6.062227385730122e-05, 9.743735846542854e-08, 1.162914220106273e-07] and 8.905501740753564e-06\n",
      "loss is 134.8809501118513\n",
      " loss is greater than learning limiter\n",
      " prev is 136.5154856681737\n",
      " diff : 1.6345355563224189\n",
      "at count 53 -> [0.019457587967678606, 1.1703822603061983e-05, -0.002903720314317046, 6.110926493044127e-05, 9.874162892355405e-08, 1.1756701624107524e-07] and 8.944617897891993e-06\n",
      "loss is 133.32163168158158\n",
      " loss is greater than learning limiter\n",
      " prev is 134.8809501118513\n",
      " diff : 1.5593184302697125\n",
      "at count 54 -> [0.019536341839631727, 1.1687752679219099e-05, -0.0030033005730340464, 6.158766130089579e-05, 1.0002750616974014e-07, 1.1881158113612986e-07] and 8.982721791593879e-06\n",
      "loss is 131.83401930086052\n",
      " loss is greater than learning limiter\n",
      " prev is 133.32163168158158\n",
      " diff : 1.4876123807210604\n",
      "at count 55 -> [0.01961305800371642, 1.167015277597098e-05, -0.003100724370124923, 6.205770386088434e-05, 1.0129538851580831e-07, 1.2002578864669255e-07] and 9.01984872045134e-06\n",
      "loss is 130.41477456895566\n",
      " loss is greater than learning limiter\n",
      " prev is 131.83401930086052\n",
      " diff : 1.419244731904854\n",
      "at count 56 -> [0.01968780751872856, 1.1651078682186842e-05, -0.003196025079933907, 6.251962325934319e-05, 1.0254566810933604e-07, 1.212103003247431e-07] and 9.056031845447614e-06\n",
      "loss is 129.06072166680457\n",
      " loss is greater than learning limiter\n",
      " prev is 130.41477456895566\n",
      " diff : 1.3540529021510963\n",
      "at count 57 -> [0.01976065714014595, 1.1630582628255742e-05, -0.0032892371453329024, 6.297364065839455e-05, 1.0377873073626575e-07, 1.2236576699032265e-07] and 9.091302386387343e-06\n",
      "loss is 127.76883814196484\n",
      " loss is greater than learning limiter\n",
      " prev is 129.06072166680457\n",
      " diff : 1.291883524839733\n",
      "at count 58 -> [0.019831669715577278, 1.1608713620742117e-05, -0.0033803958481064445, 6.34199684145101e-05, 1.0499495566592812e-07, 1.2349282847007175e-07] and 9.125689797427145e-06\n",
      "loss is 126.53624645046428\n",
      " loss is greater than learning limiter\n",
      " prev is 127.76883814196484\n",
      " diff : 1.2325916915005593\n",
      "at count 59 -> [0.01990090453813612, 1.1585517741220088e-05, -0.0034695371074513707, 6.385881069254358e-05, 1.0619471553328659e-07, 1.2459211339856259e-07] and 9.159221924010034e-06\n",
      "loss is 125.3602061554556\n",
      " loss is greater than learning limiter\n",
      " prev is 126.53624645046428\n",
      " diff : 1.1760402950086757\n",
      "at count 60 -> [0.01996841766237743, 1.1561038413239959e-05, -0.0035566973034032146, 6.4290364019902e-05, 1.0737837625379866e-07, 1.2566423907465695e-07] and 9.191925143251385e-06\n",
      "loss is 124.23810670026829\n",
      " loss is greater than learning limiter\n",
      " prev is 125.3602061554556\n",
      " diff : 1.1220994551873105\n",
      "at count 61 -> [0.020034262186918932, 1.153531664094107e-05, -0.0036419131223574845, 6.471481778732116e-05, 1.0854629696681252e-07, 1.2670981136600522e-07] and 9.22382448959683e-06\n",
      "loss is 123.16746068763317\n",
      " loss is greater than learning limiter\n",
      " prev is 124.23810670026829\n",
      " diff : 1.0706460126351232\n",
      "at count 62 -> [0.020098488507412172, 1.1508391222434925e-05, -0.003725221422170452, 6.513235470199573e-05, 1.096988300038824e-07, 1.2772942465558407e-07] and 9.25494376737031e-06\n",
      "loss is 122.1458976083174\n",
      " loss is greater than learning limiter\n",
      " prev is 123.16746068763317\n",
      " diff : 1.0215630793157686\n",
      "at count 63 -> [0.020161144543121057, 1.148029894073555e-05, -0.0038066591146053135, 6.554315119817906e-05, 1.1083632087879856e-07, 1.2872366182486824e-07] and 9.285305651650996e-06\n",
      "loss is 121.1711579716186\n",
      " loss is greater than learning limiter\n",
      " prev is 122.1458976083174\n",
      " diff : 0.9747396366987999\n",
      "at count 64 -> [0.02022227594000422, 1.1451074734705748e-05, -0.003886263063139472, 6.59473778098026e-05, 1.1195910829649446e-07, 1.2969309426884894e-07] and 9.314931778758064e-06\n",
      "loss is 120.24108779765298\n",
      " loss is greater than learning limiter\n",
      " prev is 121.1711579716186\n",
      " diff : 0.9300701739656176\n",
      "at count 65 -> [0.02028192625287613, 1.1420751852213872e-05, -0.003964069994370767, 6.634519950916321e-05, 1.1306752417831915e-07, 1.306382819386614e-07] and 9.343842827480517e-06\n",
      "loss is 119.35363343742746\n",
      " loss is greater than learning limiter\n",
      " prev is 120.24108779765298\n",
      " diff : 0.8874543602255187\n",
      "at count 66 -> [0.020340137108936222, 1.1389361987452224e-05, -0.004040116421457779, 6.673677601527971e-05, 1.1416189370145188e-07, 1.31559773408071e-07] and 9.372058592063089e-06\n",
      "loss is 118.50683669163513\n",
      " loss is greater than learning limiter\n",
      " prev is 119.35363343742746\n",
      " diff : 0.8467967457923322\n",
      "at count 67 -> [0.020396948354701596, 1.1356935404151695e-05, -0.004114438578204692, 6.712226207512386e-05, 1.152425353504928e-07, 1.3245810596050129e-07] and 9.399598047847245e-06\n",
      "loss is 117.6988302031505\n",
      " loss is greater than learning limiter\n",
      " prev is 118.50683669163513\n",
      " diff : 0.8080064884846365\n",
      "at count 68 -> [0.020452398188152986, 1.132350104623488e-05, -0.004187072362556966, 6.750180772057774e-05, 1.1630976097949122e-07, 1.333338056936707e-07] and 9.426479410366603e-06\n",
      "loss is 116.92783310151853\n",
      " loss is greater than learning limiter\n",
      " prev is 117.6988302031505\n",
      " diff : 0.7709971016319628\n",
      "at count 69 -> [0.020506523277703373, 1.1289086637278938e-05, -0.004258053288412535, 6.787555850365626e-05, 1.1736387588287557e-07, 1.3418738763924712e-07] and 9.452720188607654e-06\n",
      "loss is 116.1921468804779\n",
      " loss is greater than learning limiter\n",
      " prev is 116.92783310151853\n",
      " diff : 0.7356862210406234\n",
      "at count 70 -> [0.020559358869420216, 1.125371877000744e-05, -0.00432741644477621, 6.824365571225475e-05, 1.184051788739285e-07, 1.350193558952315e-07] and 9.478337233067873e-06\n",
      "loss is 115.4901514918127\n",
      " loss is greater than learning limiter\n",
      " prev is 116.1921468804779\n",
      " diff : 0.7019953886652104\n",
      "at count 71 -> [0.020610938883773945, 1.121742298689542e-05, -0.004395196461394291, 6.860623656843367e-05, 1.194339623696102e-07, 1.3583020376905132e-07] and 9.503346779173442e-06\n",
      "loss is 114.82030164074008\n",
      " loss is greater than learning limiter\n",
      " prev is 115.4901514918127\n",
      " diff : 0.6698498510726125\n",
      "at count 72 -> [0.020661296003044477, 1.1180223852851726e-05, -0.004461427480103443, 6.896343441103213e-05, 1.2045051248067423e-07, 1.3662041392958322e-07] and 9.527764486556569e-06\n",
      "loss is 114.18112326961118\n",
      " loss is greater than learning limiter\n",
      " prev is 114.82030164074008\n",
      " diff : 0.6391783711289065\n",
      "at count 73 -> [0.020710461750392373, 1.1142145020836117e-05, -0.0045261431312142155, 6.931537886420635e-05, 1.2145510910614625e-07, 1.3739045856653553e-07] and 9.551605474637184e-06\n",
      "loss is 113.57121021805025\n",
      " loss is greater than learning limiter\n",
      " prev is 114.18112326961118\n",
      " diff : 0.6099130515609232\n",
      "at count 74 -> [0.020758466561490038, 1.1103209291173624e-05, -0.004589376514326194, 6.96621959933145e-05, 1.2244802603134636e-07, 1.3814079955581003e-07] and 9.574884354904604e-06\n",
      "loss is 112.98922104879615\n",
      " loss is greater than learning limiter\n",
      " prev is 113.57121021805025\n",
      " diff : 0.5819891692541006\n",
      "at count 75 -> [0.020805339849509386, 1.1063438665244453e-05, -0.004651160183039861, 7.000400844941498e-05, 1.234295310287361e-07, 1.3887188862962843e-07] and 9.597615260251158e-06\n",
      "loss is 112.43387602947632\n",
      " loss is greater than learning limiter\n",
      " prev is 112.98922104879615\n",
      " diff : 0.5553450193198302\n",
      "at count 76 -> [0.020851110064174557, 1.1022854394152725e-05, -0.004711526133090772, 7.034093560350723e-05, 1.2439988596095722e-07, 1.3958416755035707e-07] and 9.619811871670914e-06\n",
      "loss is 111.90395426138436\n",
      " loss is greater than learning limiter\n",
      " prev is 112.43387602947632\n",
      " diff : 0.5299217680919668\n",
      "at count 77 -> [0.02089580474551014, 1.0981477022910737e-05, -0.004770505793485346, 7.06730936715217e-05, 1.2535934688550808e-07, 1.4027806828709447e-07] and 9.641487442602152e-06\n",
      "loss is 111.39829094706168\n",
      " loss is greater than learning limiter\n",
      " prev is 111.90395426138436\n",
      " diff : 0.5056633143226748\n",
      "at count 78 -> [0.02093945057284584, 1.0939326430616234e-05, -0.0048281300202653475, 7.100059583095682e-05, 1.2630816416057226e-07, 1.4095401319420247e-07] and 9.662654821161562e-06\n",
      "loss is 110.91577478910911\n",
      " loss is greater than learning limiter\n",
      " prev is 111.39829094706168\n",
      " diff : 0.4825161579525741\n",
      "at count 79 -> [0.020982073409576796, 1.0896421867047526e-05, -0.0048844290925704964, 7.13235523299639e-05, 1.2724658255157438e-07, 1.41612415191065e-07] and 9.683326470490868e-06\n",
      "loss is 110.45534551322005\n",
      " loss is greater than learning limiter\n",
      " prev is 110.91577478910911\n",
      " diff : 0.46042927588905513\n",
      "at count 80 -> [0.02102369834412395, 1.0852781986054515e-05, -0.00493943271070635, 7.16420705895948e-05, 1.2817484133809328e-07, 1.4225367794244921e-07] and 9.703514487412371e-06\n",
      "loss is 110.01599150891631\n",
      " loss is greater than learning limiter\n",
      " prev is 110.45534551322005\n",
      " diff : 0.4393540043037376\n",
      "at count 81 -> [0.021064349727489894, 1.0808424876082075e-05, -0.004993169995958022, 7.19562552998502e-05, 1.2909317442080973e-07, 1.4287819603892536e-07] and 9.723230619568301e-06\n",
      "loss is 109.59674758190744\n",
      " loss is greater than learning limiter\n",
      " prev is 110.01599150891631\n",
      " diff : 0.41924392700887836\n",
      "at count 82 -> [0.021104051207762455, 1.0763368088125277e-05, -0.005045669491920019, 7.226620851009893e-05, 1.3000181042820853e-07, 1.4348635517687245e-07] and 9.742486281199774e-06\n",
      "loss is 109.19669281238859\n",
      " loss is greater than learning limiter\n",
      " prev is 109.59674758190744\n",
      " diff : 0.4000547695188459\n",
      "at count 83 -> [0.02114282576187951, 1.0717628661383075e-05, -0.005096959167138866, 7.257202971437724e-05, 1.309009728227929e-07, 1.4407853233766108e-07] and 9.761292567704057e-06\n",
      "loss is 108.81494851395092\n",
      " loss is greater than learning limiter\n",
      " prev is 109.19669281238859\n",
      " diff : 0.3817442984376669\n",
      "at count 84 -> [0.021180695724934376, 1.067122314684782e-05, -0.005147066418888621, 7.287381593202354e-05, 1.317908800066014e-07, 1.446550959656599e-07] and 9.779660269093742e-06\n",
      "loss is 108.45067628809528\n",
      " loss is greater than learning limiter\n",
      " prev is 108.81494851395092\n",
      " diff : 0.36427222585564323\n",
      "at count 85 -> [0.021217682817270527, 1.0624167629042022e-05, -0.005196018077920166, 7.317166178405604e-05, 1.32671745425848e-07, 1.452164061447627e-07] and 9.797599882467942e-06\n",
      "loss is 108.10307616964229\n",
      " loss is greater than learning limiter\n",
      " prev is 108.45067628809528\n",
      " diff : 0.34760011845298777\n",
      "at count 86 -> [0.02125380816958739, 1.057647774609065e-05, -0.0052438404140436505, 7.34656595656576e-05, 1.3354377767453086e-07, 1.4576281477317606e-07] and 9.815121623593644e-06\n",
      "loss is 107.77138485859084\n",
      " loss is greater than learning limiter\n",
      " prev is 108.10307616964229\n",
      " diff : 0.33169131105145766\n",
      "at count 87 -> [0.0212890923462547, 1.0528168708296732e-05, -0.005290559142419851, 7.375589931509459e-05, 1.3440718059687938e-07, 1.4629466573624694e-07] and 9.83223543768473e-06\n",
      "loss is 107.45487403423708\n",
      " loss is greater than learning limiter\n",
      " prev is 107.77138485859084\n",
      " diff : 0.316510824353756\n",
      "at count 88 -> [0.02132355536701169, 1.0479255315369758e-05, -0.00533619943045077, 7.404246887936229e-05, 1.3526215338852844e-07, 1.4681229507714338e-07] and 9.84895100945668e-06\n",
      "loss is 107.15284874758703\n",
      " loss is greater than learning limiter\n",
      " prev is 107.45487403423708\n",
      " diff : 0.3020252866500499\n",
      "at count 89 -> [0.021357216727208056, 1.0429751972440123e-05, -0.005380785905172702, 7.432545397681938e-05, 1.3610889069632727e-07, 1.4731603116523164e-07] and 9.865277772526561e-06\n",
      "loss is 106.86464588830935\n",
      " loss is greater than learning limiter\n",
      " prev is 107.15284874758703\n",
      " diff : 0.28820285927767486\n",
      "at count 90 -> [0.02139009541672694, 1.0379672704978439e-05, -0.005424342661066479, 7.46049382570472e-05, 1.3694758271670574e-07, 1.4780619486201977e-07] and 9.881224918220422e-06\n",
      "loss is 106.58963272267864\n",
      " loss is greater than learning limiter\n",
      " prev is 106.86464588830935\n",
      " diff : 0.27501316563071043\n",
      "at count 91 -> [0.02142220993771488, 1.0329031172725677e-05, -0.005466893268209765, 7.488100335814544e-05, 1.3777841529253482e-07, 1.482830996845612e-07] and 9.896801403843535e-06\n",
      "loss is 106.32720549913395\n",
      " loss is greater than learning limiter\n",
      " prev is 106.58963272267864\n",
      " diff : 0.26242722354469095\n",
      "at count 92 -> [0.021453578321230425, 1.0277840682728674e-05, -0.0055084607807053, 7.515372896165476e-05, 1.3860157000843056e-07, 1.4874705196623203e-07] and 9.912015960463e-06\n",
      "loss is 106.07678811826057\n",
      " loss is greater than learning limiter\n",
      " prev is 106.32720549913395\n",
      " diff : 0.2504173808733867\n",
      "at count 93 -> [0.021484218142911078, 1.0226114201565398e-05, -0.005549067745326995, 7.542319284527768e-05, 1.3941722428446088e-07, 1.491983510148145e-07] and 9.92687710024699e-06\n",
      "loss is 105.83783086415667\n",
      " loss is greater than learning limiter\n",
      " prev is 106.07678811826057\n",
      " diff : 0.23895725410389446\n",
      "at count 94 -> [0.02151414653774764, 1.0173864366835308e-05, -0.005588736210332887, 7.568947093355237e-05, 1.4022555146822477e-07, 1.4963728926783414e-07] and 9.941393123400143e-06\n",
      "loss is 105.60980919430237\n",
      " loss is greater than learning limiter\n",
      " prev is 105.83783086415667\n",
      " diff : 0.22802166985430006\n",
      "at count 95 -> [0.02154338021404561, 1.0121103497982098e-05, -0.005627487734400239, 7.595263734661855e-05, 1.4102672092528106e-07, 1.5006415244511323e-07] and 9.955572124730526e-06\n",
      "loss is 105.39222258518906\n",
      " loss is greater than learning limiter\n",
      " prev is 105.60980919430237\n",
      " diff : 0.21758660911331162\n",
      "at count 96 -> [0.021571935466644845, 1.0067843606509004e-05, -0.005665343395643676, 7.621276444720157e-05, 1.418208981279115e-07, 1.504792196985139e-07] and 9.969421999879821e-06\n",
      "loss is 105.18459343110752\n",
      " loss is greater than learning limiter\n",
      " prev is 105.39222258518906\n",
      " diff : 0.2076291540815447\n",
      "at count 97 -> [0.021599828189461254, 1.0014096405640414e-05, -0.005702323800682167, 7.646992288592853e-05, 1.4260824474220917e-07, 1.508827637588562e-07] and 9.98295045124508e-06\n",
      "loss is 104.98646599360974\n",
      " loss is greater than learning limiter\n",
      " prev is 105.18459343110752\n",
      " diff : 0.1981274374977744\n",
      "at count 98 -> [0.021627073887407585, 9.959873319477943e-06, -0.0057384490937250655, 7.672418164507967e-05, 1.4338891871348868e-07, 1.5127505108000502e-07] and 9.996164993617468e-06\n",
      "loss is 104.79740539929064\n",
      " loss is greater than learning limiter\n",
      " prev is 104.98646599360974\n",
      " diff : 0.18906059431910194\n",
      "at count 99 -> [0.02165368768774448, 9.905185491693991e-06, -0.00577373896565128, 7.697560808086856e-05, 1.4416307435001934e-07, 1.5165634198012788e-07] and 1.0009072959560792e-05\n",
      "loss is 104.61699668364346\n",
      " loss is greater than learning limiter\n",
      " prev is 104.79740539929064\n",
      " diff : 0.1804087156471752\n",
      "at count 100 -> [0.021679684350907683, 9.850043793801434e-06, -0.005808212663059098, 7.722426796433627e-05, 1.4493086240508683e-07, 1.5202689078013292e-07] and 1.0021681504550266e-05\n",
      "loss is 104.44484387884987\n",
      " loss is greater than learning limiter\n",
      " prev is 104.61699668364346\n",
      " diff : 0.17215280479359762\n",
      "at count 101 -> [0.02170507828085256, 9.794458833033982e-06, -0.005841888997267224, 7.747022552093683e-05, 1.456924301573923e-07, 1.5238694593930222e-07] and 1.0033997611889885e-05\n",
      "loss is 104.28056914347086\n",
      " loss is greater than learning limiter\n",
      " prev is 104.44484387884987\n",
      " diff : 0.16427473537900994\n",
      "at count 102 -> [0.02172988353495296, 9.738440959868276e-06, -0.005874786353250264, 7.771354346888467e-05, 1.464479214898011e-07, 1.5273675018814073e-07] and 1.0046028097424935e-05\n",
      "loss is 104.12381193209941\n",
      " loss is greater than learning limiter\n",
      " prev is 104.28056914347086\n",
      " diff : 0.15675721137144194\n",
      "at count 103 -> [0.021754113833487663, 9.68200027521556e-06, -0.0059069226984943025, 7.795428305632838e-05, 1.4719747696645575e-07, 1.53076540658466e-07] and 1.0057779614064514e-05\n",
      "loss is 103.97422820312596\n",
      " loss is greater than learning limiter\n",
      " prev is 104.12381193209941\n",
      " diff : 0.14958372897345384\n",
      "at count 104 -> [0.021777782568744342, 9.625146637307987e-06, -0.005938315591760277, 7.819250409740985e-05, 1.479412339082703e-07, 1.534065490107671e-07] and 1.0069258656127463e-05\n",
      "loss is 103.83148966285943\n",
      " loss is greater than learning limiter\n",
      " prev is 103.97422820312596\n",
      " diff : 0.1427385402665351\n",
      "at count 105 -> [0.021800902813768043, 9.567889668302077e-06, -0.005968982191744741, 7.84282650072629e-05, 1.4867932646682505e-07, 1.5372700155886497e-07] and 1.0080471563523795e-05\n",
      "loss is 103.69528304432538\n",
      " loss is greater than learning limiter\n",
      " prev is 103.83148966285943\n",
      " diff : 0.1362066185340467\n",
      "at count 106 -> [0.021823487330778515, 9.510238760619611e-06, -0.005998939265629243, 7.866162283600098e-05, 1.4941188569668208e-07, 1.540381193919087e-07] and 1.0091424525782546e-05\n",
      "loss is 103.56530941914008\n",
      " loss is greater than learning limiter\n",
      " prev is 103.69528304432538\n",
      " diff : 0.12997362518530053\n",
      "at count 107 -> [0.021845548579278354, 9.452203083044247e-06, -0.006028203197510962, 7.889263330173983e-05, 1.501390396261436e-07, 1.5434011849374495e-07] and 1.0102123585935913e-05\n",
      "loss is 103.4412835409422\n",
      " loss is greater than learning limiter\n",
      " prev is 103.56530941914008\n",
      " diff : 0.1240258781978838\n",
      "at count 108 -> [0.021867098723871863, 9.393791586590367e-06, -0.00605678999670857, 7.912135082269708e-05, 1.5086091332647616e-07, 1.5463320985969924e-07] and 1.0112574644268624e-05\n",
      "loss is 103.3229332189236\n",
      " loss is greater than learning limiter\n",
      " prev is 103.4412835409422\n",
      " diff : 0.11835032201859974\n",
      "at count 109 -> [0.021888149641812616, 9.335013010159056e-06, -0.0060847153059383485, 7.934782854840798e-05, 1.5157762897962477e-07, 1.5491759961080945e-07] and 1.0122783461940672e-05\n",
      "loss is 103.20999872007678\n",
      " loss is greater than learning limiter\n",
      " prev is 103.3229332189236\n",
      " diff : 0.11293449884681195\n",
      "at count 110 -> [0.021908712930296052, 9.275875885994732e-06, -0.006111994409356624, 7.957211839009335e-05, 1.522893059444413e-07, 1.551934891055535e-07] and 1.0132755664490743e-05\n",
      "loss is 103.10223219884061\n",
      " loss is greater than learning limiter\n",
      " prev is 103.20999872007678\n",
      " diff : 0.10776652123617225\n",
      "at count 111 -> [0.02192879991351196, 9.21638854495464e-06, -0.0061386422404654225, 7.979427105021321e-05, 1.5299606082145296e-07, 1.5546107504911348e-07] and 1.0142496745227069e-05\n",
      "loss is 102.99939715287968\n",
      " loss is greater than learning limiter\n",
      " prev is 103.10223219884061\n",
      " diff : 0.10283504596092996\n",
      "at count 112 -> [0.021948421649470303, 9.156559121602304e-06, -0.006164673389879016, 8.001433605123752e-05, 1.53698007516196e-07, 1.5572054960021983e-07] and 1.0152012068511813e-05\n",
      "loss is 102.90126790380081\n",
      " loss is greater than learning limiter\n",
      " prev is 102.99939715287968\n",
      " diff : 0.09812924907886611\n",
      "at count 113 -> [0.021967588936612718, 9.096395559135059e-06, -0.006190102112949698, 8.023236176366288e-05, 1.5439525730114092e-07, 1.559721004756192e-07] and 1.0161306872944556e-05\n",
      "loss is 102.80762910166008\n",
      " loss is greater than learning limiter\n",
      " prev is 102.90126790380081\n",
      " diff : 0.09363880214073106\n",
      "at count 114 -> [0.021986312320220917, 9.035905614154802e-06, -0.006214942337251697, 8.044839543330275e-05, 1.5508791887623559e-07, 1.5621591105221042e-07] and 1.017038627444999e-05\n",
      "loss is 102.71827525216707\n",
      " loss is greater than learning limiter\n",
      " prev is 102.80762910166008\n",
      " diff : 0.0893538494930084\n",
      "at count 115 -> [0.02200460209863224, 8.975096861290393e-06, -0.006239207669922671, 8.066248320787643e-05, 1.5577609842809192e-07, 1.5645216046689295e-07] and 1.0179255269274496e-05\n",
      "loss is 102.63301026554592\n",
      " loss is greater than learning limiter\n",
      " prev is 102.71827525216707\n",
      " diff : 0.085264986621155\n",
      "at count 116 -> [0.022022468329271794, 8.913976697679311e-06, -0.00626291140486263, 8.087467016292087e-05, 1.5645989968784329e-07, 1.566810237141719e-07] and 1.0187918736895908e-05\n",
      "loss is 102.55164702605808\n",
      " loss is greater than learning limiter\n",
      " prev is 102.63301026554592\n",
      " diff : 0.08136323948784252\n",
      "at count 117 -> [0.022039920834509827, 8.852552347315629e-06, -0.006286066529790555, 8.108500032704785e-05, 1.5713942398769821e-07, 1.5690267174156436e-07] and 1.01963814428504e-05\n",
      "loss is 102.47400698124072\n",
      " loss is greater than learning limiter\n",
      " prev is 102.55164702605808\n",
      " diff : 0.07764004481735753\n",
      "at count 118 -> [0.022056969207352266, 8.790830865270686e-06, -0.006308685733159294, 8.129351670656779e-05, 1.5781477031621701e-07, 1.5711727154285115e-07] and 1.0204648041480152e-05\n",
      "loss is 102.39991974995418\n",
      " loss is greater than learning limiter\n",
      " prev is 102.47400698124072\n",
      " diff : 0.0740872312865406\n",
      "at count 119 -> [0.022073622816971804, 8.7288191417924e-06, -0.006330781410929611, 8.150026130950024e-05, 1.5848603537233713e-07, 1.5732498624921754e-07] and 1.0212723078605168e-05\n",
      "loss is 102.32922274837756\n",
      " loss is greater than learning limiter\n",
      " prev is 102.39991974995418\n",
      " diff : 0.07069700157661885\n",
      "at count 120 -> [0.02208989081408625, 8.66652390628865e-06, -0.006352365673204502, 8.17052751689901e-05, 1.591533136181731e-07, 1.5752597521832686e-07] and 1.0220610994122345e-05\n",
      "loss is 102.26176083313001\n",
      " loss is greater than learning limiter\n",
      " prev is 102.32922274837756\n",
      " diff : 0.06746191524754863\n",
      "at count 121 -> [0.02210578213619049, 8.603951731199767e-06, -0.006373450350725124, 8.19085983661475e-05, 1.5981669733061678e-07, 1.577203941213699e-07] and 1.0228316124534711e-05\n",
      "loss is 102.19738596073053\n",
      " loss is greater than learning limiter\n",
      " prev is 102.26176083313001\n",
      " diff : 0.06437487239948325\n",
      "at count 122 -> [0.022121305512647838, 8.541109035764753e-06, -0.006394047001229843, 8.211027005232876e-05, 1.6047627665176272e-07, 1.57908395028133e-07] and 1.0235842705413505e-05\n",
      "loss is 102.13595686265019\n",
      " loss is greater than learning limiter\n",
      " prev is 102.19738596073053\n",
      " diff : 0.06142909808033892\n",
      "at count 123 -> [0.022136469469646217, 8.478002089685566e-06, -0.006414166915678068, 8.231032847087442e-05, 1.6113213963818434e-07, 1.5809012649012647e-07] and 1.0243194873795635e-05\n",
      "loss is 102.07733873523951\n",
      " loss is greater than learning limiter\n",
      " prev is 102.13595686265019\n",
      " diff : 0.058618127410682064\n",
      "at count 124 -> [0.022151282335024242, 8.41463701669346e-06, -0.00643382112434067, 8.250881097832005e-05, 1.6178437230908482e-07, 1.5826573362181558e-07] and 1.0250376670518834e-05\n",
      "loss is 102.02140294385221\n",
      " loss is greater than learning limiter\n",
      " prev is 102.07733873523951\n",
      " diff : 0.05593579138729865\n",
      "at count 125 -> [0.022165752242971893, 8.351019798021102e-06, -0.006453020402758888, 8.270575406509489e-05, 1.624330586933476e-07, 1.5843535817999448e-07] and 1.025739204249673e-05\n",
      "loss is 101.96802674051156\n",
      " loss is greater than learning limiter\n",
      " prev is 102.02140294385221\n",
      " diff : 0.05337620334064752\n",
      "at count 126 -> [0.022179887138610233, 8.287156275783962e-06, -0.006471775277573712, 8.290119337572217e-05, 1.6307828087550989e-07, 1.5859913864134391e-07] and 1.0264244844935885e-05\n",
      "loss is 101.91709299450373\n",
      " loss is greater than learning limiter\n",
      " prev is 101.96802674051156\n",
      " diff : 0.05093374600782852\n",
      "at count 127 -> [0.022193694782454306, 8.223052156274207e-06, -0.006490096032227802, 8.309516372853521e-05, 1.6372011904068328e-07, 1.5875721027821193e-07] and 1.0270938843496746e-05\n",
      "loss is 101.86848993529982\n",
      " loss is greater than learning limiter\n",
      " prev is 101.91709299450373\n",
      " diff : 0.048603059203912835\n",
      "at count 128 -> [0.022207182754763143, 8.15871301317017e-06, -0.006507992712542064, 8.32876991349219e-05, 1.6435865151844415e-07, 1.5890970523265724e-07] and 1.0277477716400341e-05\n",
      "loss is 101.82211090724813\n",
      " loss is greater than learning limiter\n",
      " prev is 101.86848993529982\n",
      " diff : 0.04637902805168892\n",
      "at count 129 -> [0.022220358459780525, 8.094144290664253e-06, -0.006525475132169049, 8.347883281811062e-05, 1.6499395482571677e-07, 1.59056752588793e-07] and 1.0283865056482433e-05\n",
      "loss is 101.77785413549373\n",
      " loss is greater than learning limiter\n",
      " prev is 101.82211090724813\n",
      " diff : 0.04425677175440512\n",
      "at count 130 -> [0.02223322912987, 8.029351306511944e-06, -0.0065425528779253575, 8.36685972315093e-05, 1.6562610370867158e-07, 1.5919847844346924e-07] and 1.0290104373196783e-05\n",
      "loss is 101.73562250261118\n",
      " loss is greater than learning limiter\n",
      " prev is 101.77785413549373\n",
      " diff : 0.04223163288254739\n",
      "at count 131 -> [0.02224580182954745, 7.964339255004528e-06, -0.006559235315005279, 8.385702407660953e-05, 1.6625517118366034e-07, 1.5933500597533057e-07] and 1.0296199094569069e-05\n",
      "loss is 101.69532333546232\n",
      " loss is greater than learning limiter\n",
      " prev is 101.73562250261118\n",
      " diff : 0.04029916714885928\n",
      "at count 132 -> [0.022258083459414327, 7.899113209867863e-06, -0.006575531592077887, 8.404414432046687e-05, 1.6688122857721002e-07, 1.5946645551228573e-07] and 1.0302152569102924e-05\n",
      "loss is 101.65686820180923\n",
      " loss is greater than learning limiter\n",
      " prev is 101.69532333546232\n",
      " diff : 0.038455133653087614\n",
      "at count 133 -> [0.02227008075999451, 7.833678127089527e-06, -0.00659145064626983, 8.422998821276819e-05, 1.6750434556509612e-07, 1.595929445974244e-07] and 1.0307968067639512e-05\n",
      "loss is 101.62017271623563\n",
      " loss is greater than learning limiter\n",
      " prev is 101.65686820180923\n",
      " diff : 0.03669548557360258\n",
      "at count 134 -> [0.02228180031547761, 7.768038847676477e-06, -0.006607001208036055, 8.441458530249658e-05, 1.6812459021051635e-07, 1.5971458805341637e-07] and 1.0313648785171963e-05\n",
      "loss is 101.58515635495074\n",
      " loss is greater than learning limiter\n",
      " prev is 101.62017271623563\n",
      " diff : 0.03501636128488883\n",
      "at count 135 -> [0.022293248557371442, 7.7022001003453e-06, -0.006622191805920691, 8.459796445420391e-05, 1.6874202900138518e-07, 1.5983149804542717e-07] and 1.0319197842615951e-05\n",
      "loss is 101.55174227906964\n",
      " loss is greater than learning limiter\n",
      " prev is 101.58515635495074\n",
      " diff : 0.03341407588109746\n",
      "at count 136 -> [0.022304431768066197, 7.636166504146997e-06, -0.006637030771210311, 8.478015386390087e-05, 1.6935672688676857e-07, 1.5994378414258376e-07] and 1.0324618288537645e-05\n",
      "loss is 101.51985716598135\n",
      " loss is greater than learning limiter\n",
      " prev is 101.55174227906964\n",
      " diff : 0.03188511308829334\n",
      "at count 137 -> [0.022315356084312784, 7.569942571028177e-06, -0.00665152624248178, 8.496118107457404e-05, 1.6996874731247889e-07, 1.6005155337802302e-07] and 1.0329913100840189e-05\n",
      "loss is 101.48943104843399\n",
      " loss is greater than learning limiter\n",
      " prev is 101.51985716598135\n",
      " diff : 0.03042611754736413\n",
      "at count 138 -> [0.022326027500617726, 7.503532708330447e-06, -0.006665686170046868, 8.514107299133915e-05, 1.7057815225584882e-07, 1.6015491030755553e-07] and 1.0335085188409837e-05\n",
      "loss is 101.46039716098564\n",
      " loss is greater than learning limiter\n",
      " prev is 101.48943104843399\n",
      " diff : 0.02903388744834956\n",
      "at count 139 -> [0.022336451872556826, 7.436941221229712e-06, -0.006679518320295794, 8.531985589623942e-05, 1.7118500225970302e-07, 1.6025395706697556e-07] and 1.0340137392722839e-05\n",
      "loss is 101.43269179348138\n",
      " loss is greater than learning limiter\n",
      " prev is 101.46039716098564\n",
      " diff : 0.027705367504253786\n",
      "at count 140 -> [0.022346634920009805, 7.3701723151170225e-06, -0.006693030279941837, 8.549755546269784e-05, 1.7178935646554572e-07, 1.6034879342804873e-07] and 1.0345072489414082e-05\n",
      "loss is 101.40625415123607\n",
      " loss is greater than learning limiter\n",
      " prev is 101.43269179348138\n",
      " diff : 0.02643764224531253\n",
      "at count 141 -> [0.022356582230317994, 7.303230097922538e-06, -0.006706229460169124, 8.567419676963145e-05, 1.7239127264598222e-07, 1.6043951685320704e-07] and 1.0349893189808521e-05\n",
      "loss is 101.38102622161593\n",
      " loss is greater than learning limiter\n",
      " prev is 101.40625415123607\n",
      " diff : 0.02522792962014364\n",
      "at count 142 -> [0.02236629926136709, 7.236118582384131e-06, -0.006719123100685681, 8.58498043152363e-05, 1.7299080723639176e-07, 1.6052622254898103e-07] and 1.0354602142416342e-05\n",
      "loss is 101.3569526467265\n",
      " loss is greater than learning limiter\n",
      " prev is 101.38102622161593\n",
      " diff : 0.024073574889428073\n",
      "at count 143 -> [0.022375791344596913, 7.168841688262076e-06, -0.0067317182736837886, 8.602440203045052e-05, 1.7358801536586882e-07, 1.606090035181979e-07] and 1.0359201934392788e-05\n",
      "loss is 101.33398060192634\n",
      " loss is greater than learning limiter\n",
      " prev is 101.3569526467265\n",
      " diff : 0.0229720448001558\n",
      "at count 144 -> [0.022385063687940035, 7.101403244501237e-06, -0.006744021887709677, 8.619801329210359e-05, 1.7418295088744984e-07, 1.606879506109738e-07] and 1.0363695092963562e-05\n",
      "loss is 101.31205967990006\n",
      " loss is greater than learning limiter\n",
      " prev is 101.33398060192634\n",
      " diff : 0.021920922026282597\n",
      "at count 145 -> [0.022394121378691106, 7.033806991342102e-06, -0.006756040691444528, 8.637066093575922e-05, 1.7477566640764134e-07, 1.6076315257452793e-07] and 1.0368084086816648e-05\n",
      "loss is 101.2911417800377\n",
      " loss is greater than learning limiter\n",
      " prev is 101.31205967990006\n",
      " diff : 0.02091789986235426\n",
      "at count 146 -> [0.02240296938630859, 6.966056582381968e-06, -0.006767781277398756, 8.654236726825908e-05, 1.7536621331526584e-07, 1.6083469610184558e-07] and 1.0372371327461423e-05\n",
      "loss is 101.27118100287494\n",
      " loss is greater than learning limiter\n",
      " prev is 101.2911417800377\n",
      " diff : 0.019960777162765453\n",
      "at count 147 -> [0.022411612565150653, 6.898155586587544e-06, -0.006779250085521476, 8.671315407997463e-05, 1.7595464180964098e-07, 1.6090266587921636e-07] and 1.0376559170555843e-05\n",
      "loss is 101.2521335493661\n",
      " loss is greater than learning limiter\n",
      " prev is 101.27118100287494\n",
      " diff : 0.019047453508832746\n",
      "at count 148 -> [0.022420055657146783, 6.830107490260194e-06, -0.006790453406727043, 8.688304265677388e-05, 1.7654100092810733e-07, 1.6096714463267367e-07] and 1.0380649917202521e-05\n",
      "loss is 101.23395762476414\n",
      " loss is greater than learning limiter\n",
      " prev is 101.2521335493661\n",
      " diff : 0.01817592460196238\n",
      "at count 149 -> [0.02242830329440678, 6.761915698954998e-06, -0.00680139738634052, 8.705205379170992e-05, 1.7712533857291983e-07, 1.6102821317336042e-07] and 1.0384645815214433e-05\n",
      "loss is 101.2166133469017\n",
      " loss is greater than learning limiter\n",
      " prev is 101.23395762476414\n",
      " diff : 0.017344277862449076\n",
      "at count 150 -> [0.022436360001768633, 6.693583539354781e-06, -0.0068120880274638845, 8.722020779643771e-05, 1.777077015375173e-07, 1.6108595044184597e-07] and 1.0388549060351014e-05\n",
      "loss is 101.20006265866873\n",
      " loss is greater than learning limiter\n",
      " prev is 101.2166133469017\n",
      " diff : 0.016550688232968014\n",
      "at count 151 -> [0.02244423019928677, 6.625114261100218e-06, -0.006822531194264746, 8.738752451236568e-05, 1.7828813553218477e-07, 1.6114043355141817e-07] and 1.0392361797525368e-05\n",
      "loss is 101.1842692444938\n",
      " loss is greater than learning limiter\n",
      " prev is 101.20006265866873\n",
      " diff : 0.015793414174922304\n",
      "at count 152 -> [0.022451918204662163, 6.55651103857709e-06, -0.00683273261518934, 8.755402332154819e-05, 1.78866685209122e-07, 1.6119173783037438e-07] and 1.0396086121983268e-05\n",
      "loss is 101.16919845065013\n",
      " loss is greater than learning limiter\n",
      " prev is 101.1842692444938\n",
      " diff : 0.01507079384367671\n",
      "at count 153 -> [0.022459428235615662, 6.4877769726617435e-06, -0.006842697886101479, 8.771972315732519e-05, 1.794433941869326e-07, 1.6123993686333424e-07] and 1.0399724080454652e-05\n",
      "loss is 101.15481720920661\n",
      " loss is greater than learning limiter\n",
      " prev is 101.16919845065013\n",
      " diff : 0.014381241443516046\n",
      "at count 154 -> [0.022466764412205936, 6.418915092425762e-06, -0.006852432473349163, 8.788464251471479e-05, 1.800183050745464e-07, 1.61285102531597e-07] and 1.0403277672278266e-05\n",
      "loss is 101.14109396545967\n",
      " loss is greater than learning limiter\n",
      " prev is 101.15481720920661\n",
      " diff : 0.013723243746937897\n",
      "at count 155 -> [0.02247393075909339, 6.3499283568008365e-06, -0.006861941716760475, 8.804879946056465e-05, 1.8059145949458863e-07, 1.6132730505256524e-07] and 1.0406748850500093e-05\n",
      "loss is 101.12799860868563\n",
      " loss is greater than learning limiter\n",
      " prev is 101.14109396545967\n",
      " diff : 0.01309535677404483\n",
      "at count 156 -> [0.02248093120775128, 6.2808196562048e-06, -0.006871230832570362, 8.821221164346794e-05, 1.8116289810620845e-07, 1.6136661301825662e-07] and 1.0410139522946207e-05\n",
      "loss is 101.11550240606272\n",
      " loss is greater than learning limiter\n",
      " prev is 101.12799860868563\n",
      " diff : 0.012496202622912733\n",
      "at count 157 -> [0.02248776959862539, 6.211591814129752e-06, -0.006880304916279898, 8.837489630344911e-05, 1.817326606273794e-07, 1.614030934329247e-07] and 1.0413451553270643e-05\n",
      "loss is 101.10357793961632\n",
      " loss is greater than learning limiter\n",
      " prev is 101.11550240606272\n",
      " diff : 0.011924466446401993\n",
      "at count 158 -> [0.022494449683243368, 6.142247588693183e-06, -0.006889168945449536, 8.853687028142511e-05, 1.8230078585668383e-07, 1.6143681174980947e-07] and 1.0416686761978896e-05\n",
      "loss is 101.09219904604976\n",
      " loss is greater than learning limiter\n",
      " prev is 101.10357793961632\n",
      " diff : 0.01137889356655819\n",
      "at count 159 -> [0.02250097512627504, 6.072789674152979e-06, -0.006897827782427885, 8.869815002844722e-05, 1.828673116945934e-07, 1.6146783190703737e-07] and 1.041984692742762e-05\n",
      "loss is 101.08134075932873\n",
      " loss is greater than learning limiter\n",
      " prev is 101.09219904604976\n",
      " diff : 0.010858286721031618\n",
      "at count 160 -> [0.02250734950754478, 6.003220702387177e-06, -0.0069062861770174695, 8.885875161472857e-05, 1.8343227516425707e-07, 1.614962163626908e-07] and 1.0422933786801078e-05\n",
      "loss is 101.07097925589304\n",
      " loss is greater than learning limiter\n",
      " prev is 101.08134075932873\n",
      " diff : 0.010361503435689201\n",
      "at count 161 -> [0.022513576323997125, 5.93354324433929e-06, -0.006914548769078908, 8.901869073846235e-05, 1.8399571243180828e-07, 1.6152202612906622e-07] and 1.0425949037064912e-05\n",
      "loss is 101.06109180237561\n",
      " loss is greater than learning limiter\n",
      " prev is 101.07097925589304\n",
      " diff : 0.009887453517421818\n",
      "at count 162 -> [0.02251965899161669, 5.863759811430028e-06, -0.006922620091074941, 8.917798273443572e-05, 1.8455765882620203e-07, 1.615453208061394e-07] and 1.0428894335897752e-05\n",
      "loss is 101.0516567057138\n",
      " loss is greater than learning limiter\n",
      " prev is 101.06109180237561\n",
      " diff : 0.009435096661817965\n",
      "at count 163 -> [0.022525600847303505, 5.793872856936217e-06, -0.006930504570555668, 8.933664258244395e-05, 1.85118148858593e-07, 1.6156615861425633e-07] and 1.0431771302601199e-05\n",
      "loss is 101.04265326554417\n",
      " loss is greater than learning limiter\n",
      " prev is 101.0516567057138\n",
      " diff : 0.009003440169621513\n",
      "at count 164 -> [0.022531405150704797, 5.72388477733767e-06, -0.006938206532586355, 8.949468491550972e-05, 1.8567721624126523e-07, 1.615845964260676e-07] and 1.0434581518988667e-05\n",
      "loss is 101.03406172877492\n",
      " loss is greater than learning limiter\n",
      " prev is 101.04265326554417\n",
      " diff : 0.008591536769259278\n",
      "at count 165 -> [0.022537075086004246, 5.6537979136327805e-06, -0.006945730202119119, 8.965212402791183e-05, 1.862348939061238e-07, 1.6160068979772377e-07] and 1.0437326530253621e-05\n",
      "loss is 101.02586324623813\n",
      " loss is greater than learning limiter\n",
      " prev is 101.03406172877492\n",
      " diff : 0.008198482536784013\n",
      "at count 166 -> [0.022542613763669714, 5.583614552623572e-06, -0.006953079706309797, 8.980897388302795e-05, 1.8679121402275846e-07, 1.616144929993489e-07] and 1.0440007845817642e-05\n",
      "loss is 101.01803983132451\n",
      " loss is greater than learning limiter\n",
      " prev is 101.02586324623813\n",
      " diff : 0.007823414913616489\n",
      "at count 167 -> [0.022548024222160422, 5.513336928170914e-06, -0.0069602590767812444, 8.99652481209957e-05, 1.873462080160892e-07, 1.6162605904480854e-07] and 1.0442626940158838e-05\n",
      "loss is 101.01057432051257\n",
      " loss is greater than learning limiter\n",
      " prev is 101.01803983132451\n",
      " diff : 0.007465510811940135\n",
      "at count 168 -> [0.02255330942959452, 5.442967222420612e-06, -0.006967272251834298, 9.012096006619618e-05, 1.8789990658360362e-07, 1.616354397207892e-07] and 1.0445185253621026e-05\n",
      "loss is 101.00345033570187\n",
      " loss is greater than learning limiter\n",
      " prev is 101.01057432051257\n",
      " diff : 0.007123984810704087\n",
      "at count 169 -> [0.022558472285377985, 5.372507567001051e-06, -0.0069741230786076166, 9.027612273456423e-05, 1.8845233971219516e-07, 1.6164268561520449e-07] and 1.0447684193204163e-05\n",
      "loss is 100.99665224827285\n",
      " loss is greater than learning limiter\n",
      " prev is 101.00345033570187\n",
      " diff : 0.006798087429018551\n",
      "at count 170 -> [0.022563515621795736, 5.30196004419305e-06, -0.00698081531518757, 9.043074884072928e-05, 1.890035366946117e-07, 1.6164784614494384e-07] and 1.0450125133336439e-05\n",
      "loss is 100.99016514478856\n",
      " loss is greater than learning limiter\n",
      " prev is 100.99665224827285\n",
      " diff : 0.0064871034842894915\n",
      "at count 171 -> [0.02256844220556587, 5.231326688072599e-06, -0.006987352632669327, 9.058485080499091e-05, 1.8955352614552347e-07, 1.616509695829792e-07] and 1.0452509416628474e-05\n",
      "loss is 100.9839747942675\n",
      " loss is greater than learning limiter\n",
      " prev is 100.99016514478856\n",
      " diff : 0.006190350521066534\n",
      "at count 172 -> [0.02257325473935785, 5.160609485627082e-06, -0.0069937386171702726, 9.073844076013273e-05, 1.9010233601721906e-07, 1.6165210308484414e-07] and 1.0454838354610041e-05\n",
      "loss is 100.97806761695385\n",
      " loss is greater than learning limiter\n",
      " prev is 100.9839747942675\n",
      " diff : 0.005907177313645207\n",
      "at count 173 -> [0.022577955863275517, 5.0898103778456334e-06, -0.0069999767717968476, 9.089153055807858e-05, 1.9064999361493817e-07, 1.6165129271450019e-07] and 1.0457113228449696e-05\n",
      "loss is 100.97243065451636\n",
      " loss is greater than learning limiter\n",
      " prev is 100.97806761695385\n",
      " diff : 0.005636962437492343\n",
      "at count 174 -> [0.022582548156305726, 5.0189312607842126e-06, -0.0070060705185658895, 9.104413177639446e-05, 1.9119652561184936e-07, 1.616485834696044e-07] and 1.0459335289657749e-05\n",
      "loss is 100.96705154161103\n",
      " loss is greater than learning limiter\n",
      " prev is 100.97243065451636\n",
      " diff : 0.005379112905330885\n",
      "at count 175 -> [0.022587034137733396, 4.94797398660599e-06, -0.007012023200281525, 9.119625572464e-05, 1.9174195806368132e-07, 1.6164401930619217e-07] and 1.0461505760772933e-05\n",
      "loss is 100.96191847874637\n",
      " loss is greater than learning limiter\n",
      " prev is 100.96705154161103\n",
      " diff : 0.0051330628646582\n",
      "at count 176 -> [0.022591416268523797, 4.8769403645976285e-06, -0.007017838082368634, 9.134791345057281e-05, 1.9228631642301523e-07, 1.616376431627887e-07] and 1.0463625836033178e-05\n",
      "loss is 100.9570202063882\n",
      " loss is greater than learning limiter\n",
      " prev is 100.96191847874637\n",
      " diff : 0.004898272358175859\n",
      "at count 177 -> [0.022595696952672786, 4.8058321621620046e-06, -0.007023518354663897, 9.149911574620913e-05, 1.928296255532466e-07, 1.6162949698396238e-07] and 1.0465696682030825e-05\n",
      "loss is 100.95234598025105\n",
      " loss is greater than learning limiter\n",
      " prev is 100.9570202063882\n",
      " diff : 0.004674226137140636\n",
      "at count 178 -> [0.02259987853852578, 4.734651105787935e-06, -0.007029067133165397, 9.164987315374418e-05, 1.9337190974222364e-07, 1.616196217433329e-07] and 1.0467719438352674e-05\n",
      "loss is 100.94788554772042\n",
      " loss is greater than learning limiter\n",
      " prev is 100.95234598025105\n",
      " diff : 0.004460432530635217\n",
      "at count 179 -> [0.02260396332006616, 4.6633988819974336e-06, -0.007034487461741743, 9.180019597133543e-05, 1.939131927155702e-07, 1.6160805746604695e-07] and 1.0469695218205207e-05\n",
      "loss is 100.9436291253524\n",
      " loss is greater than learning limiter\n",
      " prev is 100.94788554772042\n",
      " diff : 0.004256422368015933\n",
      "at count 180 -> [0.02260795353817382, 4.592077138271019e-06, -0.0070397823138016345, 9.195009425875194e-05, 1.9445349764970024e-07, 1.6159484325073364e-07] and 1.0471625109025319e-05\n",
      "loss is 100.93956737740405\n",
      " loss is greater than learning limiter\n",
      " prev is 100.9436291253524\n",
      " diff : 0.004061747948355787\n",
      "at count 181 -> [0.022611851381854613, 4.5206874839515934e-06, -0.007044954593924801, 9.209957784289295e-05, 1.9499284718453087e-07, 1.6158001729095159e-07] and 1.0473510173076913e-05\n",
      "loss is 100.93569139534712\n",
      " loss is greater than learning limiter\n",
      " prev is 100.93956737740405\n",
      " diff : 0.003875982056925409\n",
      "at count 182 -> [0.02261565898944124, 4.4492314911273816e-06, -0.007050007139455192, 9.224865632317859e-05, 1.9553126343590145e-07, 1.6156361689613978e-07] and 1.0475351448033672e-05\n",
      "loss is 100.93199267831675\n",
      " loss is greater than learning limiter\n",
      " prev is 100.93569139534712\n",
      " diff : 0.0036987170303746097\n",
      "at count 183 -> [0.022619378449766407, 4.377710695494415e-06, -0.007054942722057301, 9.23973390768159e-05, 1.9606876800770502e-07, 1.6154567851208325e-07] and 1.0477149947548334e-05\n",
      "loss is 100.92846311445754\n",
      " loss is greater than learning limiter\n",
      " prev is 100.93199267831675\n",
      " diff : 0.003529563859203222\n",
      "at count 184 -> [0.022623011803308758, 4.306126597199044e-06, -0.007059764049236472, 9.254563526394289e-05, 1.9660538200373887e-07, 1.6152623774090528e-07] and 1.0478906661808789e-05\n",
      "loss is 100.92509496312164\n",
      " loss is greater than learning limiter\n",
      " prev is 100.92846311445754\n",
      " diff : 0.0033681513359056225\n",
      "at count 185 -> [0.022626561043312298, 4.234480661660939e-06, -0.00706447376582402, 9.269355383265346e-05, 1.9714112603928103e-07, 1.6150532936059653e-07] and 1.048062255808129e-05\n",
      "loss is 100.92188083788182\n",
      " loss is greater than learning limiter\n",
      " prev is 100.92509496312164\n",
      " diff : 0.0032141252398218967\n",
      "at count 186 -> [0.02263002811687988, 4.162774320377022e-06, -0.007069074455427977, 9.2841103523906e-05, 1.976760202523985e-07, 1.6148298734409247e-07] and 1.0482298581241097e-05\n",
      "loss is 100.91881369032166\n",
      " loss is greater than learning limiter\n",
      " prev is 100.92188083788182\n",
      " diff : 0.0030671475601593556\n",
      "at count 187 -> [0.02263341492604137, 4.091008971706797e-06, -0.007073568641850273, 9.298829287631844e-05, 1.9821008431499396e-07, 1.6145924487790886e-07] and 1.0483935654290816e-05\n",
      "loss is 100.91588679456845\n",
      " loss is greater than learning limiter\n",
      " prev is 100.91881369032166\n",
      " diff : 0.002926895753205372\n",
      "at count 188 -> [0.02263672332879708, 4.019185981639477e-06, -0.007077958790471103, 9.313513023085215e-05, 1.9874333744359669e-07, 1.614341343803462e-07] and 1.048553467886675e-05\n",
      "loss is 100.91309373253289\n",
      " loss is greater than learning limiter\n",
      " prev is 100.91588679456845\n",
      " diff : 0.0027930620355647306\n",
      "at count 189 -> [0.022639955140137055, 3.947306684543358e-06, -0.007082247309601261, 9.328162373538752e-05, 1.992757984099036e-07, 1.6140768751927245e-07] and 1.0487096535733525e-05\n",
      "loss is 100.91042837982484\n",
      " loss is greater than learning limiter\n",
      " prev is 100.91309373253289\n",
      " diff : 0.002665352708049795\n",
      "at count 190 -> [0.022643112133036724, 3.875372383897837e-06, -0.007086436551803174, 9.342778134919353e-05, 1.9980748555107646e-07, 1.6137993522949443e-07] and 1.0488622085267251e-05\n",
      "loss is 100.9078848923124\n",
      " loss is greater than learning limiter\n",
      " prev is 100.91042837982484\n",
      " diff : 0.002543487512440379\n",
      "at count 191 -> [0.02264619603942954, 3.8033843530084815e-06, -0.007090528815181358, 9.35736108472939e-05, 2.0033841677980064e-07, 1.6135090772972686e-07] and 1.0490112167927512e-05\n",
      "loss is 100.90545769329839\n",
      " loss is greater than learning limiter\n",
      " prev is 100.9078848923124\n",
      " diff : 0.002427199014007897\n",
      "at count 192 -> [0.02264920855115706, 3.7313438357055404e-06, -0.0070945263446430045, 9.371911982473213e-05, 2.00868609594111e-07, 1.613206345391688e-07] and 1.0491567604718415e-05\n",
      "loss is 100.90314146128152\n",
      " loss is greater than learning limiter\n",
      " prev is 100.90545769329839\n",
      " diff : 0.002316232016866593\n",
      "at count 193 -> [0.02265215132089706, 3.6592520470262827e-06, -0.0070984313331293935, 9.386431570073783e-05, 2.0139808108699043e-07, 1.6128914449369623e-07] and 1.0492989197638974e-05\n",
      "loss is 100.90093111827787\n",
      " loss is greater than learning limiter\n",
      " prev is 100.90314146128152\n",
      " diff : 0.0022103430036537475\n",
      "at count 194 -> [0.022655025963070136, 3.587110173881536e-06, -0.007102245922818801, 9.400920572279659e-05, 2.0192684795574606e-07, 1.6125646576167986e-07] and 1.0494377730123062e-05\n",
      "loss is 100.8988218186785\n",
      " loss is greater than learning limiter\n",
      " prev is 100.90093111827787\n",
      " diff : 0.0021092995993683417\n",
      "at count 195 -> [0.022657834054725334, 3.5149193757067957e-06, -0.007105972206301562, 9.415379697062561e-05, 2.0245492651116857e-07, 1.6122262585943685e-07] and 1.0495733967469173e-05\n",
      "loss is 100.89680893861413\n",
      " loss is greater than learning limiter\n",
      " prev is 100.8988218186785\n",
      " diff : 0.0020128800643703926\n",
      "at count 196 -> [0.022660577136405263, 3.4426807850982577e-06, -0.007109612227727939, 9.429809636005735e-05, 2.0298233268647944e-07, 1.6118765166632472e-07] and 1.0497058657260243e-05\n",
      "loss is 100.89488806580967\n",
      " loss is greater than learning limiter\n",
      " prev is 100.89680893861413\n",
      " diff : 0.0019208728044617374\n",
      "at count 197 -> [0.02266325671299119, 3.370395508434124e-06, -0.007113167983929418, 9.444211064683322e-05, 2.0350908204607095e-07, 1.6115156943948597e-07] and 1.049835252977375e-05\n",
      "loss is 100.89305498990039\n",
      " loss is greater than learning limiter\n",
      " prev is 100.89488806580967\n",
      " diff : 0.0018330759092748394\n",
      "at count 198 -> [0.022665874254528564, 3.298064626481527e-06, -0.007116641425514051, 9.458584643030947e-05, 2.0403518979404414e-07, 1.6111440482825132e-07] and 1.0499616298382308e-05\n",
      "loss is 100.8913056931928\n",
      " loss is greater than learning limiter\n",
      " prev is 100.89305498990039\n",
      " diff : 0.0017492967075867227\n",
      "at count 199 -> [0.02266843119703342, 3.2256891949894e-06, -0.007120034457936443, 9.472931015707729e-05, 2.0456067078254888e-07, 1.6107618288820968e-07] and 1.0500850659945009e-05\n",
      "loss is 100.88963634184759\n",
      " loss is greater than learning limiter\n",
      " prev is 100.8913056931928\n",
      " diff : 0.0016693513452139541\n",
      "at count 200 -> [0.022670928943280138, 3.153270245267623e-06, -0.007123348942542977, 9.487250812449912e-05, 2.0508553951993096e-07, 1.6103692809495223e-07] and 1.050205629518968e-05\n",
      "loss is 100.8880432774662\n",
      " loss is greater than learning limiter\n",
      " prev is 100.88963634184759\n",
      " diff : 0.0015930643813959477\n",
      "at count 201 -> [0.022673368863570936, 3.0808087847527615e-06, -0.007126586697592835, 9.501544648416307e-05, 2.0560981017869065e-07, 1.609966643574984e-07] and 1.0503233869086309e-05\n",
      "loss is 100.88652300906267\n",
      " loss is greater than learning limiter\n",
      " prev is 100.8880432774662\n",
      " diff : 0.001520268403524483\n",
      "at count 202 -> [0.02267575229648757, 3.0083057975607064e-06, -0.007129749499255399, 9.515813124525742e-05, 2.06133496603257e-07, 1.6095541503141126e-07] and 1.0504384031211821e-05\n",
      "loss is 100.88507220540212\n",
      " loss is greater than learning limiter\n",
      " prev is 100.88652300906267\n",
      " diff : 0.0014508036605462848\n",
      "at count 203 -> [0.02267808054962562, 2.9357622450265207e-06, -0.00713283908258455, 9.530056827786691e-05, 2.0665661231758216e-07, 1.6091320293160903e-07] and 1.0505507416106405e-05\n",
      "loss is 100.88368768769178\n",
      " loss is greater than learning limiter\n",
      " prev is 100.88507220540212\n",
      " diff : 0.0013845177103490869\n",
      "at count 204 -> [0.02268035490031178, 2.8631790662317893e-06, -0.007135857142470424, 9.544276331619286e-05, 2.071791705325598e-07, 1.6087005034488014e-07] and 1.0506604643621596e-05\n",
      "loss is 100.88236642260294\n",
      " loss is greater than learning limiter\n",
      " prev is 100.88368768769178\n",
      " diff : 0.0013212650888334565\n",
      "at count 205 -> [0.022682576596304554, 2.7905571785197592e-06, -0.007138805334569134, 9.55847219616986e-05, 2.0770118415327214e-07, 1.608259790421087e-07] and 1.0507676319260303e-05\n",
      "loss is 100.88110551561613\n",
      " loss is greater than learning limiter\n",
      " prev is 100.88236642260294\n",
      " diff : 0.0012609069868148026\n",
      "at count 206 -> [0.022684746856478713, 2.7178974779985548e-06, -0.0071416852762109725, 9.572644968618225e-05, 2.082226657860688e-07, 1.6078101029021678e-07] and 1.0508723034508952e-05\n",
      "loss is 100.8799022046685\n",
      " loss is greater than learning limiter\n",
      " prev is 100.88110551561613\n",
      " diff : 0.001203310947630598\n",
      "at count 207 -> [0.02268686687149392, 2.6452008400327444e-06, -0.007144498547287587, 9.586795183477817e-05, 2.087436277454821e-07, 1.607351648638304e-07] and 1.0509745367161956e-05\n",
      "loss is 100.8787538540919\n",
      " loss is greater than learning limiter\n",
      " prev is 100.8799022046685\n",
      " diff : 0.001148350576599455\n",
      "at count 208 -> [0.02268893780444786, 2.572468119723526e-06, -0.007147246691118624, 9.600923362888914e-05, 2.0926408206098208e-07, 1.6068846305667535e-07] and 1.0510743881638663e-05\n",
      "loss is 100.87765794882996\n",
      " loss is greater than learning limiter\n",
      " prev is 100.8787538540919\n",
      " diff : 0.0010959052619341492\n",
      "at count 209 -> [0.02269096079151428, 2.4997001523777975e-06, -0.007149931215298309, 9.615030016905048e-05, 2.0978404048357535e-07, 1.6064092469270927e-07] and 1.0511719129292976e-05\n",
      "loss is 100.87661208891758\n",
      " loss is greater than learning limiter\n",
      " prev is 100.87765794882996\n",
      " diff : 0.0010458599123808199\n",
      "at count 210 -> [0.022692936942566234, 2.4268977539663687e-06, -0.007152553592522435, 9.629115643772802e-05, 2.1030351449225104e-07, 1.6059256913699613e-07] and 1.0512671648715791e-05\n",
      "loss is 100.87561398421559\n",
      " loss is greater than learning limiter\n",
      " prev is 100.87661208891758\n",
      " diff : 0.0009981047019920197\n",
      "at count 211 -> [0.022694867341784903, 2.3540617215715634e-06, -0.007155115261396198, 9.643180730205118e-05, 2.1082251530027752e-07, 1.605434153063288e-07] and 1.0513601966030448e-05\n",
      "loss is 100.87466144938378\n",
      " loss is greater than learning limiter\n",
      " prev is 100.87561398421559\n",
      " diff : 0.0009525348318106808\n",
      "at count 212 -> [0.022696753048254328, 2.2811928338244646e-06, -0.007157617627223343, 9.657225751648285e-05, 2.1134105386135346e-07, 1.604934816796059e-07] and 1.0514510595181336e-05\n",
      "loss is 100.87375239908562\n",
      " loss is greater than learning limiter\n",
      " prev is 100.87466144938378\n",
      " diff : 0.0009090502981621285\n",
      "at count 213 -> [0.022698595096542377, 2.2082918513320325e-06, -0.007160062062777039, 9.671251172542749e-05, 2.1185914087561652e-07, 1.6044278630796826e-07] and 1.051539803821582e-05\n",
      "loss is 100.87288484341207\n",
      " loss is greater than learning limiter\n",
      " prev is 100.87375239908562\n",
      " diff : 0.0008675556735511236\n",
      "at count 214 -> [0.022700394497268257, 2.1353595170943382e-06, -0.007162449909052904, 9.685257446577874e-05, 2.1237678679551282e-07, 1.6039134682470098e-07] and 1.0516264785559639e-05\n",
      "loss is 100.87205688351506\n",
      " loss is greater than learning limiter\n",
      " prev is 100.87288484341207\n",
      " diff : 0.0008279598970091229\n",
      "at count 215 -> [0.022702152237656887, 2.062396556912139e-06, -0.007164782476004609, 9.699245016940819e-05, 2.1289400183153058e-07, 1.6033918045490604e-07] and 1.0517111316285926e-05\n",
      "loss is 100.87126670743945\n",
      " loss is greater than learning limiter\n",
      " prev is 100.87205688351506\n",
      " diff : 0.0007901760756112708\n",
      "at count 216 -> [0.022703869282080447, 1.989403679785017e-06, -0.007167061043262447, 9.713214316559647e-05, 2.1341079595780117e-07, 1.602863040249511e-07] and 1.0517938098378012e-05\n",
      "loss is 100.87051258614777\n",
      " loss is greater than learning limiter\n",
      " prev is 100.87126670743945\n",
      " diff : 0.000754121291677734\n",
      "at count 217 -> [0.022705546572587384, 1.916381578300301e-06, -0.007169286860835273, 9.727165768340807e-05, 2.1392717891757032e-07, 1.6023273397169964e-07] and 1.051874558898613e-05\n",
      "loss is 100.86979286972495\n",
      " loss is greater than learning limiter\n",
      " prev is 100.87051258614777\n",
      " diff : 0.0007197164228216479\n",
      "at count 218 -> [0.022707185029419173, 1.8433309290129889e-06, -0.00717146114979619, 9.741099785401116e-05, 2.1444316022854278e-07, 1.6017848635152734e-07] and 1.0519534234678192e-05\n",
      "loss is 100.86910598375857\n",
      " loss is greater than learning limiter\n",
      " prev is 100.86979286972495\n",
      " diff : 0.0006868859663740068\n",
      "at count 219 -> [0.022708785551515117, 1.77025239281687e-06, -0.007173585102952364, 9.755016771294382e-05, 2.1495874918810328e-07, 1.601235768491297e-07] and 1.0520304471684751e-05\n",
      "loss is 100.8684504258824\n",
      " loss is greater than learning limiter\n",
      " prev is 100.86910598375857\n",
      " diff : 0.0006555578761719971\n",
      "at count 220 -> [0.02271034901700546, 1.69714661530706e-06, -0.007175659885499343, 9.768917120232763e-05, 2.154739548784165e-07, 1.6006802078612603e-07] and 1.0521056726138296e-05\n",
      "loss is 100.86782476248096\n",
      " loss is greater than learning limiter\n",
      " prev is 100.8684504258824\n",
      " diff : 0.0006256634014363271\n",
      "at count 221 -> [0.022711876283693085, 1.6240142271341434e-06, -0.007177686635660213, 9.782801217303022e-05, 2.1598878617140922e-07, 1.6001183312946392e-07] and 1.0521791414307005e-05\n",
      "loss is 100.86722762554254\n",
      " loss is greater than learning limiter\n",
      " prev is 100.86782476248096\n",
      " diff : 0.0005971369384241143\n",
      "at count 222 -> [0.022713368189524064, 1.550855844350115e-06, -0.007179666465309974, 9.796669438677755e-05, 2.1650325173363686e-07, 1.5995502849962945e-07] and 1.0522508942823092e-05\n",
      "loss is 100.86665770965706\n",
      " loss is greater than learning limiter\n",
      " prev is 100.86722762554254\n",
      " diff : 0.0005699158854781672\n",
      "at count 223 -> [0.02271482555304731, 1.477672068746318e-06, -0.007181600460585454, 9.81052215182175e-05, 2.170173600310376e-07, 1.598976211786672e-07] and 1.0523209708905864e-05\n",
      "loss is 100.86611376915175\n",
      " loss is greater than learning limiter\n",
      " prev is 100.86665770965706\n",
      " diff : 0.0005439405053095925\n",
      "at count 224 -> [0.022716249173863598, 1.4044634881835545e-06, -0.007183489682481107, 9.824359715693555e-05, 2.175311193335763e-07, 1.5983962511801458e-07] and 1.0523894100579616e-05\n",
      "loss is 100.86559461535765\n",
      " loss is greater than learning limiter\n",
      " prev is 100.86611376915175\n",
      " diff : 0.0005191537941016122\n",
      "at count 225 -> [0.022717639833064178, 1.331230676914558e-06, -0.00718533516743102, 9.838182480942385e-05, 2.18044537719781e-07, 1.5978105394615502e-07] and 1.052456249688648e-05\n",
      "loss is 100.865099114\n",
      " loss is greater than learning limiter\n",
      " prev is 100.86559461535765\n",
      " diff : 0.000495501357647754\n",
      "at count 226 -> [0.022718998293659247, 1.2579741958989988e-06, -0.0071871379278774464, 9.851990790100468e-05, 2.1855762308117442e-07, 1.597219209760937e-07] and 1.0525215268094349e-05\n",
      "loss is 100.86462618271042\n",
      " loss is greater than learning limiter\n",
      " prev is 100.865099114\n",
      " diff : 0.0004729312895790372\n",
      "at count 227 -> [0.022720325300996498, 1.1846945931111996e-06, -0.00718889895282618, 9.865784977770945e-05, 2.1907038312660276e-07, 1.5966223921266042e-07] and 1.052585277589999e-05\n",
      "loss is 100.86417478865106\n",
      " loss is greater than learning limiter\n",
      " prev is 100.86462618271042\n",
      " diff : 0.0004513940593682264\n",
      "at count 228 -> [0.02272162158316998, 1.1113924038407246e-06, -0.00719061920838907, 9.879565370811411e-05, 2.1958282538646446e-07, 1.5960202135964346e-07] and 1.0526475373627448e-05\n",
      "loss is 100.8637439462475\n",
      " loss is greater than learning limiter\n",
      " prev is 100.86417478865106\n",
      " diff : 0.00043084240355995007\n",
      "at count 229 -> [0.022722887851419488, 1.0380681509860143e-06, -0.007192299638313983, 9.893332288513217e-05, 2.2009495721684093e-07, 1.5954127982675812e-07] and 1.0527083406421864e-05\n",
      "loss is 100.86333271502689\n",
      " loss is greater than learning limiter\n",
      " prev is 100.8637439462475\n",
      " diff : 0.0004112312206103752\n",
      "at count 230 -> [0.022724124800520724, 9.647223453412197e-07, -0.007193941164502492, 9.907086042776617e-05, 2.2060678580353172e-07, 1.594800267364541e-07] and 1.0527677211438807e-05\n",
      "loss is 100.86294019755326\n",
      " loss is greater than learning limiter\n",
      " prev is 100.86333271502689\n",
      " diff : 0.00039251747362811784\n",
      "at count 231 -> [0.022725333109166412, 8.913554858763988e-07, -0.007195544687515584, 9.920826938281856e-05, 2.211183181659963e-07, 1.594182739305653e-07] and 1.0528257118029213e-05\n",
      "loss is 100.86256553745879\n",
      " loss is greater than learning limiter\n",
      " prev is 100.86294019755326\n",
      " diff : 0.00037466009446518456\n",
      "at count 232 -> [0.022726513440338582, 8.179680600112264e-07, -0.007197111087067662, 9.934555272656305e-05, 2.216295611612046e-07, 1.5935603297680546e-07] and 1.0528823447920058e-05\n",
      "loss is 100.86220791756351\n",
      " loss is greater than learning limiter\n",
      " prev is 100.86256553745879\n",
      " diff : 0.00035761989528282356\n",
      "at count 233 -> [0.022727666441672265, 7.445605438823673e-07, -0.007198641222509109, 9.948271336637725e-05, 2.221405214873984e-07, 1.5929331517511387e-07] and 1.0529376515390836e-05\n",
      "loss is 100.86186655808363\n",
      " loss is greater than learning limiter\n",
      " prev is 100.86220791756351\n",
      " diff : 0.0003413594798757913\n",
      "at count 234 -> [0.022728792745810748, 6.711334026046611e-07, -0.0072001359332976855, 9.961975414233748e-05, 2.2265120568776545e-07, 1.5923013156385374e-07] and 1.052991662744596e-05\n",
      "loss is 100.86154071491785\n",
      " loss is greater than learning limiter\n",
      " prev is 100.86186655808363\n",
      " diff : 0.0003258431657826577\n",
      "at count 235 -> [0.022729892970752598, 5.976870905262604e-07, -0.007201596039459012, 9.975667782877672e-05, 2.2316162015402905e-07, 1.5916649292586733e-07] and 1.0530444083983166e-05\n",
      "loss is 100.86122967801732\n",
      " loss is greater than learning limiter\n",
      " prev is 100.86154071491785\n",
      " diff : 0.0003110369005270286\n",
      "at count 236 -> [0.022730967720190686, 5.242220514778629e-07, -0.007203022342036391, 9.989348713580645e-05, 2.2367177112995395e-07, 1.5910240979439087e-07] and 1.0530959177958023e-05\n",
      "loss is 100.86093276982471\n",
      " loss is greater than learning limiter\n",
      " prev is 100.86122967801732\n",
      " diff : 0.0002969081926096351\n",
      "at count 237 -> [0.02273201758384332, 4.507387190161744e-07, -0.00720441562353022, 0.00010003018471080327, 2.2418166471477147e-07, 1.590378924588326e-07] and 1.0531462195544634e-05\n",
      "loss is 100.86064934378958\n",
      " loss is greater than learning limiter\n",
      " prev is 100.86093276982471\n",
      " diff : 0.0002834260351392004\n",
      "at count 238 -> [0.022733043137777756, 3.7723751666173607e-07, -0.00720577664832723, 0.00010016677313986114, 2.2469130686652525e-07, 1.589729509704169e-07] and 1.0531953416292614e-05\n",
      "loss is 100.86037878294765\n",
      " loss is greater than learning limiter\n",
      " prev is 100.86064934378958\n",
      " diff : 0.0002705608419262262\n",
      "at count 239 -> [0.022734044944726174, 3.03718858131246e-07, -0.007207106163119789, 0.00010030325494920994, 2.2520070340533943e-07, 1.58907595147698e-07] and 1.0532433113280441e-05\n",
      "loss is 100.8601204985683\n",
      " loss is greater than learning limiter\n",
      " prev is 100.86037878294765\n",
      " diff : 0.00025828437934194426\n",
      "at count 240 -> [0.022735023554394388, 2.3018314756450283e-07, -0.007208404897315497, 0.0001004396326066013, 2.2570986001661134e-07, 1.588418345819458e-07] and 1.053290155326525e-05\n",
      "loss is 100.85987392886173\n",
      " loss is greater than learning limiter\n",
      " prev is 100.8601204985683\n",
      " diff : 0.0002465697065758832\n",
      "at count 241 -> [0.02273597950376337, 1.5663077974609686e-07, -0.0072096735634373035, 0.0001005759085226622, 2.2621878225413026e-07, 1.5877567864240727e-07] and 1.053335899682916e-05\n",
      "loss is 100.85963853774635\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85987392886173\n",
      " diff : 0.00023539111538184443\n",
      "at count 242 -> [0.022736913317383825, 8.306214032196786e-08, -0.007210912857514362, 0.00010071208505221735, 2.2672747554312382e-07, 1.5870913648144586e-07] and 1.0533805698522225e-05\n",
      "loss is 100.85941381367232\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85963853774635\n",
      " diff : 0.00022472407403029138\n",
      "at count 243 -> [0.022737825507663918, 9.477606010951303e-09, -0.007212123459463832, 0.00010084816449558094, 2.2723594518323408e-07, 1.5864221703956204e-07] and 1.0534241907002052e-05\n",
      "loss is 100.8591992684989\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85941381367232\n",
      " diff : 0.00021454517342078816\n",
      "at count 244 -> [0.022738716575150357, -6.412245518857354e-08, -0.00721330603346385, 0.00010098414909981846, 2.2774419635142465e-07, 1.585749290502976e-07] and 1.0534667865170217e-05\n",
      "loss is 100.85899443642394\n",
      " loss is greater than learning limiter\n",
      " prev is 100.8591992684989\n",
      " diff : 0.00020483207495658462\n",
      "at count 245 -> [0.022739587008802978, -1.3773768379681758e-07, -0.007214461228317859, 0.00010112004105997934, 2.2825223410482046e-07, 1.585072810450264e-07] and 1.05350838103055e-05\n",
      "loss is 100.8587988729602\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85899443642394\n",
      " diff : 0.00019556346374827172\n",
      "at count 246 -> [0.022740437286262957, -2.1136772865539088e-07, -0.007215589677810515, 0.00010125584252030102, 2.2876006338348187e-07, 1.5843928135763432e-07] and 1.0535489974194049e-05\n",
      "loss is 100.85861215396118\n",
      " loss is greater than learning limiter\n",
      " prev is 100.8587988729602\n",
      " diff : 0.00018671899901789857\n",
      "at count 247 -> [0.022741267874114814, -2.8501224673467443e-07, -0.007216692001055343, 0.00010139155557538515, 2.2926768901311453e-07, 1.583709381290907e-07] and 1.0535886583256517e-05\n",
      "loss is 100.8584338746897\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85861215396118\n",
      " diff : 0.00017827927148061917\n",
      "at count 248 -> [0.022742079228142368, -3.586709029456519e-07, -0.007217768802834353, 0.00010152718227134665, 2.297751157077165e-07, 1.5830225931191408e-07] and 1.0536273858672273e-05\n",
      "loss is 100.85826364893022\n",
      " loss is greater than learning limiter\n",
      " prev is 100.8584338746897\n",
      " diff : 0.0001702257594757839\n",
      "at count 249 -> [0.022742871793578746, -4.3234336995609687e-07, -0.007218820673929794, 0.00010166272460693603, 2.3028234807216415e-07, 1.5823325267453458e-07] and 1.0536652016500731e-05\n",
      "loss is 100.85810110814049\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85826364893022\n",
      " diff : 0.00016254078973076957\n",
      "at count 250 -> [0.022743646005350622, -5.060293280110154e-07, -0.007219848191448222, 0.00010179818453463585, 2.3078939060473806e-07, 1.5816392580555526e-07] and 1.0537021267799861e-05\n",
      "loss is 100.85794590064364\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85810110814049\n",
      " diff : 0.0001552074968458328\n",
      "at count 251 -> [0.022744402288316794, -5.797284647572448e-07, -0.007220851919137075, 0.00010193356396173174, 2.3129624769959064e-07, 1.580942861179148e-07] and 1.053738181874197e-05\n",
      "loss is 100.85779769085596\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85794590064364\n",
      " diff : 0.0001482097876817079\n",
      "at count 252 -> [0.02274514105750124, -6.534404750721125e-07, -0.007221832407693915, 0.00010206886475135861, 2.3180292364915653e-07, 1.5802434085295392e-07] and 1.0537733870726798e-05\n",
      "loss is 100.85765615855017\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85779769085596\n",
      " diff : 0.00014153230578983766\n",
      "at count 253 -> [0.022745862718320773, -7.271650608960613e-07, -0.007222790195068512, 0.00010220408872352271, 2.3230942264650715e-07, 1.5795409708438748e-07] and 1.0538077620491994e-05\n",
      "loss is 100.8575209981534\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85765615855017\n",
      " diff : 0.00013516039676630953\n",
      "at count 254 -> [0.022746567666807445, -8.009019310691501e-07, -0.007223725806757936, 0.00010233923765609996, 2.3281574878765083e-07, 1.578835617221849e-07] and 1.0538413260221033e-05\n",
      "loss is 100.85739191807586\n",
      " loss is greater than learning limiter\n",
      " prev is 100.8575209981534\n",
      " diff : 0.00012908007754219852\n",
      "at count 255 -> [0.022747256289825775, -8.746508011713372e-07, -0.007224639756094814, 0.00010247431328581124, 2.333219060737797e-07, 1.5781274151636055e-07] and 1.0538740977648647e-05\n",
      "loss is 100.85726864007196\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85739191807586\n",
      " diff : 0.0001232780038975534\n",
      "at count 256 -> [0.02274792896528497, -9.484113933664632e-07, -0.007225532544528925, 0.00010260931730917502, 2.3382789841346452e-07, 1.5774164306067642e-07] and 1.0539060956163796e-05\n",
      "loss is 100.8571508986284\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85726864007196\n",
      " diff : 0.00011774144356024863\n",
      "at count 257 -> [0.022748586062346234, -1.0221834362498436e-06, -0.007226404661902263, 0.00010274425138343802, 2.343337296247987e-07, 1.5767027279625924e-07] and 1.0539373374910271e-05\n",
      "loss is 100.85703844038207\n",
      " loss is greater than learning limiter\n",
      " prev is 100.8571508986284\n",
      " diff : 0.00011245824633476786\n",
      "at count 258 -> [0.022749227941625254, -1.0959666646993906e-06, -0.007227256586717742, 0.00010287911712748428, 2.348394034374927e-07, 1.575986370151336e-07] and 1.0539678408884965e-05\n",
      "loss is 100.85693102356481\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85703844038207\n",
      " diff : 0.00010741681725789931\n",
      "at count 259 -> [0.022749854955390027, -1.1697608197301808e-06, -0.007228088786401678, 0.00010301391612272317, 2.3534492349492006e-07, 1.5752674186367356e-07] and 1.0539976229033865e-05\n",
      "loss is 100.85682841747152\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85693102356481\n",
      " diff : 0.00010260609329293402\n",
      "at count 260 -> [0.022750467447754098, -1.2435656483523887e-06, -0.007228901717560195, 0.0001031486499139569, 2.358502933561158e-07, 1.574545933459741e-07] and 1.0540267002345823e-05\n",
      "loss is 100.8567304019545\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85682841747152\n",
      " diff : 9.801551702537381e-05\n",
      "at count 261 -> [0.022751065754865313, -1.3173809034325106e-06, -0.007229695826229694, 0.00010328332001022792, 2.3635551649772886e-07, 1.5738219732714485e-07] and 1.0540550891944157e-05\n",
      "loss is 100.85663676694065\n",
      " loss is greater than learning limiter\n",
      " prev is 100.8567304019545\n",
      " diff : 9.36350138402986e-05\n",
      "at count 262 -> [0.02275165020509022, -1.3912063435577987e-06, -0.0072304715481215295, 0.00010341792788564668, 2.3686059631592914e-07, 1.5730955953652717e-07] and 1.0540828057176138e-05\n",
      "loss is 100.85654731197006\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85663676694065\n",
      " diff : 8.945497059187346e-05\n",
      "at count 263 -> [0.022752221119194192, -1.465041732903835e-06, -0.007231229308861013, 0.00010355247498020029, 2.373655361282705e-07, 1.5723668557083712e-07] and 1.054109865370039e-05\n",
      "loss is 100.85646184575577\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85654731197006\n",
      " diff : 8.546621428706658e-05\n",
      "at count 264 -> [0.02275277881051738, -1.5388868411051697e-06, -0.007231969524220893, 0.00010368696270054237, 2.378703391755106e-07, 1.5716358089723548e-07] and 1.0541362833572287e-05\n",
      "loss is 100.85638018576492\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85646184575577\n",
      " diff : 8.165999085463227e-05\n",
      "at count 265 -> [0.022753323585146604, -1.6127414431289562e-06, -0.007232692600349425, 0.0001038213924207647, 2.383750086233886e-07, 1.5709025085632677e-07] and 1.0541620745327356e-05\n",
      "loss is 100.8563021578172\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85638018576492\n",
      " diff : 7.802794772260313e-05\n",
      "at count 266 -> [0.022753855742083242, -1.6866053191515075e-06, -0.007233398933993165, 0.00010395576548315099, 2.3887954756436167e-07, 1.5701670066508882e-07] and 1.0541872534062755e-05\n",
      "loss is 100.85622759570339\n",
      " loss is greater than learning limiter\n",
      " prev is 100.8563021578172\n",
      " diff : 7.456211380940658e-05\n",
      "at count 267 -> [0.02275437557340726, -1.7604782544377134e-06, -0.007234088912714602, 0.00010409008319891315, 2.393839590193015e-07, 1.5694293541973453e-07] and 1.0542118341516874e-05\n",
      "loss is 100.85615634082018\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85622759570339\n",
      " diff : 7.125488320980367e-05\n",
      "at count 268 -> [0.022754883364437425, -1.8343600392232478e-06, -0.007234762915104757, 0.00010422434684891058, 2.3988824593915117e-07, 1.5686896009850738e-07] and 1.0542358306147071e-05\n",
      "loss is 100.85608824182171\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85615634082018\n",
      " diff : 6.809899846871303e-05\n",
      "at count 269 -> [0.02275537939388782, -1.9082504685995023e-06, -0.007235421310990857, 0.00010435855768435278, 2.403924112065441e-07, 1.5679477956441227e-07] and 1.054259256320564e-05\n",
      "loss is 100.85602315428889\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85608824182171\n",
      " diff : 6.508753281764257e-05\n",
      "at count 270 -> [0.02275586393402073, -1.982149342401185e-06, -0.007236064461639203, 0.0001044927169274857, 2.408964576373853e-07, 1.5672039856788308e-07] and 1.054282124481399e-05\n",
      "loss is 100.8559609404105\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85602315428889\n",
      " diff : 6.221387839389081e-05\n",
      "at count 271 -> [0.022756337250795986, -2.0560564650965178e-06, -0.007236692719953345, 0.00010462682577226217, 2.414003879823958e-07, 1.5664582174938858e-07] and 1.0543044480035126e-05\n",
      "loss is 100.85590146868219\n",
      " loss is greater than learning limiter\n",
      " prev is 100.8559609404105\n",
      " diff : 5.947172830644831e-05\n",
      "at count 272 -> [0.022756799604016872, -2.1299716456799806e-06, -0.007237306430667663, 0.0001047608853849969, 2.4190420492862183e-07, 1.5657105364197785e-07] and 1.0543262394944444e-05\n",
      "loss is 100.8558446136177\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85590146868219\n",
      " diff : 5.6855064485716866e-05\n",
      "at count 273 -> [0.022757251247472625, -2.2038946975675336e-06, -0.007237905930536472, 0.00010489489690500623, 2.424079111009086e-07, 1.5649609867376697e-07] and 1.0543475112698892e-05\n",
      "loss is 100.85579025547278\n",
      " loss is greater than learning limiter\n",
      " prev is 100.8558446136177\n",
      " diff : 5.4358144922161955e-05\n",
      "at count 274 -> [0.022757692429077655, -2.277825438494267e-06, -0.007238491548518749, 0.00010502886144523306, 2.4291150906334024e-07, 1.5642096117036798e-07] and 1.0543682753604527e-05\n",
      "loss is 100.85573827998304\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85579025547278\n",
      " diff : 5.197548973967514e-05\n",
      "at count 275 -> [0.022758123391007537, -2.3517636904144218e-06, -0.007239063605958583, 0.00010516278009285742, 2.4341500132064646e-07, 1.5634564535726177e-07] and 1.0543885435182504e-05\n",
      "loss is 100.85568857811315\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85573827998304\n",
      " diff : 4.970186989794456e-05\n",
      "at count 276 -> [0.02275854436983186, -2.425709279403719e-06, -0.007239622416761445, 0.00010529665390989283, 2.4391839031957617e-07, 1.5627015536211586e-07] and 1.0544083272233551e-05\n",
      "loss is 100.85564104581732\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85568857811315\n",
      " diff : 4.7532295823771165e-05\n",
      "at count 277 -> [0.022758955596644002, -2.4996620355639583e-06, -0.007240168287566384, 0.00010543048393376882, 2.4442167845024e-07, 1.5619449521704878e-07] and 1.0544276376900935e-05\n",
      "loss is 100.85559558381092\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85564104581732\n",
      " diff : 4.5462006397656296e-05\n",
      "at count 278 -> [0.022759357297187876, -2.5736217929298182e-06, -0.007240701517914231, 0.00010556427117790004, 2.449248680474211e-07, 1.5611866886084169e-07] and 1.054446485873198e-05\n",
      "loss is 100.85555209735286\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85559558381092\n",
      " diff : 4.348645806828699e-05\n",
      "at count 279 -> [0.022759749691981784, -2.6475883893778156e-06, -0.007241222400411913, 0.00010569801663224218, 2.454279613918562e-07, 1.5604268014109914e-07] and 1.0544648824738161e-05\n",
      "loss is 100.85551049603754\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85555209735286\n",
      " diff : 4.160131531705247e-05\n",
      "at count 280 -> [0.022760132996439367, -2.721561666537374e-06, -0.007241731220892961, 0.00010583172126383498, 2.459309607114863e-07, 1.559665328163596e-07] and 1.0544828379453805e-05\n",
      "loss is 100.85547069359663\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85551049603754\n",
      " diff : 3.9802440909397774e-05\n",
      "at count 281 -> [0.02276050742098778, -2.795541469703951e-06, -0.007242228258574306, 0.00010596538601733273, 2.4643386818267924e-07, 1.5589023055815722e-07] and 1.054500362499343e-05\n",
      "loss is 100.8554326077092\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85547069359663\n",
      " diff : 3.80858874251544e-05\n",
      "at count 282 -> [0.022760873171183107, -2.8695276477541717e-06, -0.007242713786209438, 0.00010609901181552244, 2.469366859314232e-07, 1.5581377695303587e-07] and 1.054517466110776e-05\n",
      "loss is 100.8553961598221\n",
      " loss is greater than learning limiter\n",
      " prev is 100.8554326077092\n",
      " diff : 3.644788709777913e-05\n",
      "at count 283 -> [0.02276123044782314, -2.9435200530629333e-06, -0.007243188070238032, 0.00010623259955983003, 2.4743941603449314e-07, 1.5573717550451647e-07] and 1.0545341585238447e-05\n",
      "loss is 100.85536127497625\n",
      " loss is greater than learning limiter\n",
      " prev is 100.8553961598221\n",
      " diff : 3.488484586000595e-05\n",
      "at count 284 -> [0.022761579447057506, -3.0175185414224252e-06, -0.007243651370932095, 0.00010636615013081481, 2.479420605205898e-07, 1.556604296350189e-07] and 1.0545504492571523e-05\n",
      "loss is 100.85532788164306\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85536127497625\n",
      " diff : 3.3393333183084906e-05\n",
      "at count 285 -> [0.022761920360495277, -3.0915229719630222e-06, -0.007244103942538744, 0.0001064996643886525, 2.4844462137145264e-07, 1.5558354268773933e-07] and 1.0545663476089607e-05\n",
      "loss is 100.85529591156748\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85532788164306\n",
      " diff : 3.197007558242149e-05\n",
      "at count 286 -> [0.022762253375310074, -3.1655332070760126e-06, -0.00724454603341967, 0.00010663314317360712, 2.4894710052294706e-07, 1.5550651792848415e-07] and 1.0545818626622915e-05\n",
      "loss is 100.85526529961734\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85529591156748\n",
      " diff : 3.06119501374269e-05\n",
      "at count 287 -> [0.022762578674342736, -3.2395491123381126e-06, -0.007244977886187377, 0.00010676658730649187, 2.4944949986612606e-07, 1.5542935854746139e-07] and 1.0545970032899088e-05\n",
      "loss is 100.85523598364168\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85526529961734\n",
      " diff : 2.9315975666577287e-05\n",
      "at count 288 -> [0.022762896436201625, -3.313570556437728e-06, -0.007245399737838266, 0.00010689999758911938, 2.499518212482678e-07, 1.5535206766103074e-07] and 1.0546117781591858e-05\n",
      "loss is 100.85520790433333\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85523598364168\n",
      " diff : 2.8079308350470455e-05\n",
      "at count 289 -> [0.02276320683536059, -3.387597411102923e-06, -0.0072458118198826405, 0.0001070333748047416, 2.5045406647388884e-07, 1.55274648313413e-07] and 1.0546261957368597e-05\n",
      "loss is 100.85518100509906\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85520790433333\n",
      " diff : 2.689923427112717e-05\n",
      "at count 290 -> [0.022763510042254693, -3.46162955103106e-06, -0.007246214358471702, 0.00010716671971847939, 2.509562373057338e-07, 1.5519710347836002e-07] and 1.0546402642936754e-05\n",
      "loss is 100.85515523193597\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85518100509906\n",
      " diff : 2.57731630881608e-05\n",
      "at count 291 -> [0.022763806223373678, -3.5356668538200603e-06, -0.007246607574521601, 0.00010730003307774227, 2.5145833546574276e-07, 1.5511943606078588e-07] and 1.0546539919089233e-05\n",
      "loss is 100.85513053331155\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85515523193597\n",
      " diff : 2.4698624415009363e-05\n",
      "at count 292 -> [0.022764095541353318, -3.6097091999012638e-06, -0.007246991683834623, 0.00010743331561263837, 2.5196036263599535e-07, 1.5504164889836046e-07] and 1.0546673864748709e-05\n",
      "loss is 100.85510686005227\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85513053331155\n",
      " diff : 2.3673259278211845e-05\n",
      "at count 293 -> [0.022764378155064616, -3.6837564724738394e-06, -0.0072473668972175675, 0.00010756656803637497, 2.524623204596338e-07, 1.5496374476306592e-07] and 1.0546804557010917e-05\n",
      "loss is 100.85508416523443\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85510686005227\n",
      " diff : 2.2694817843671444e-05\n",
      "at count 294 -> [0.02276465421970096, -3.7578085574407136e-06, -0.0072477334205973834, 0.0001076997910456497, 2.5296421054176416e-07, 1.5488572636271741e-07] and 1.0546932071186946e-05\n",
      "loss is 100.85506240408223\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85508416523443\n",
      " diff : 2.1761152197541378e-05\n",
      "at count 295 -> [0.022764923886863256, -3.831865343345982e-06, -0.007248091455134131, 0.00010783298532103274, 2.534660344503366e-07, 1.548075963424484e-07] and 1.0547056480844552e-05\n",
      "loss is 100.85504153386789\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85506240408223\n",
      " diff : 2.0870214342494364e-05\n",
      "at count 296 -> [0.022765187304643077, -3.905926721313772e-06, -0.007248441197331328, 0.0001079661515273401, 2.5396779371700596e-07, 1.547293572861615e-07] and 1.0547177857848511e-05\n",
      "loss is 100.85502151381993\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85504153386789\n",
      " diff : 2.002004795542689e-05\n",
      "at count 297 -> [0.022765444617703915, -3.979992584988517e-06, -0.007248782839143741, 0.00010809929031399837, 2.5446948983797136e-07, 1.5465101171794614e-07] and 1.054729627240004e-05\n",
      "loss is 100.85500230503243\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85502151381993\n",
      " diff : 1.9208787506386216e-05\n",
      "at count 298 -> [0.022765695967360517, -4.054062830476618e-06, -0.007249116568082683, 0.00010823240231540089, 2.5497112427479746e-07, 1.5457256210346292e-07] and 1.0547411793075309e-05\n",
      "loss is 100.85498387037914\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85500230503243\n",
      " diff : 1.843465328477123e-05\n",
      "at count 299 -> [0.022765941491656425, -4.128137356289448e-06, -0.007249442567318872, 0.0001083654881512558, 2.554726984552159e-07, 1.5449401085129624e-07] and 1.0547524486863055e-05\n",
      "loss is 100.85496617443397\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85498387037914\n",
      " diff : 1.7695945174978078e-05\n",
      "at count 300 -> [0.02276618132543968, -4.2022160632876885e-06, -0.007249761015782907, 0.00010849854842692595, 2.5597421377390845e-07, 1.5441536031427554e-07] and 1.054763441920133e-05\n",
      "loss is 100.85494918339087\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85496617443397\n",
      " diff : 1.6991043096936664e-05\n",
      "at count 301 -> [0.0227664156004368, -4.276298854626939e-06, -0.007250072088263422, 0.00010863158373376098, 2.564756715932723e-07, 1.543366127907659e-07] and 1.0547741654013398e-05\n",
      "loss is 100.85493286499162\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85494918339087\n",
      " diff : 1.6318399246983972e-05\n",
      "at count 302 -> [0.022766644445325043, -4.350385635704598e-06, -0.007250375955502969, 0.00010876459464942172, 2.5697707324416724e-07, 1.5425777052592884e-07] and 1.0547846253742797e-05\n",
      "loss is 100.85491718845397\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85493286499162\n",
      " diff : 1.567653765732757e-05\n",
      "at count 303 -> [0.022766867985802988, -4.4244763141079645e-06, -0.007250672784291671, 0.00010889758173819704, 2.574784200266457e-07, 1.5417883571295386e-07] and 1.0547948279387594e-05\n",
      "loss is 100.85490212440418\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85491718845397\n",
      " diff : 1.5064049790680656e-05\n",
      "at count 304 -> [0.022767086344659484, -4.4985707995635506e-06, -0.007250962737558722, 0.00010903054555131341, 2.579797132106659e-07, 1.5409981049426152e-07] and 1.0548047790533849e-05\n",
      "loss is 100.85488764481295\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85490212440418\n",
      " diff : 1.4479591229132893e-05\n",
      "at count 305 -> [0.022767299641841012, -4.572669003887558e-06, -0.007251245974461758, 0.00010916348662723724, 2.584809540367885e-07, 1.5402069696267873e-07] and 1.0548144845388294e-05\n",
      "loss is 100.85487372293338\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85488764481295\n",
      " diff : 1.3921879570943929e-05\n",
      "at count 306 -> [0.022767507994517478, -4.6467708409375065e-06, -0.007251522650474161, 0.00010929640549197032, 2.5898214371685716e-07, 1.5394149716258667e-07] and 1.0548239500810266e-05\n",
      "loss is 100.85486033324278\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85487372293338\n",
      " diff : 1.3389690593612613e-05\n",
      "at count 307 -> [0.02276771151714649, -4.72087622656498e-06, -0.0072517929174703505, 0.00010942930265933826, 2.5948328343466326e-07, 1.538622130910423e-07] and 1.05483318123429e-05\n",
      "loss is 100.85484745138625\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85486033324278\n",
      " diff : 1.2881856534363578e-05\n",
      "at count 308 -> [0.02276791032153613, -4.794985078569468e-06, -0.0072520569238090846, 0.00010956217863127247, 2.5998437434659515e-07, 1.5378284669887375e-07] and 1.0548421834243588e-05\n",
      "loss is 100.85483505412377\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85484745138625\n",
      " diff : 1.2397262480590143e-05\n",
      "at count 309 -> [0.02276810451690631, -4.869097316653274e-06, -0.00725231481441485, 0.00010969503389808539, 2.6048541758227265e-07, 1.5370339989175062e-07] and 1.0548509619513754e-05\n",
      "loss is 100.85482311927849\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85483505412377\n",
      " diff : 1.19348452756185e-05\n",
      "at count 310 -> [0.02276829420994866, -4.943212862377467e-06, -0.007252566730857355, 0.00010982786893873947, 2.609864142451666e-07, 1.5362387453122915e-07] and 1.0548595219927916e-05\n",
      "loss is 100.85481162568843\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85482311927849\n",
      " diff : 1.1493590065470016e-05\n",
      "at count 311 -> [0.022768479504885072, -5.017331639118858e-06, -0.00725281281142919, 0.00010996068422110998, 2.6148736541320434e-07, 1.5354427243577343e-07] and 1.0548678686062097e-05\n",
      "loss is 100.85480055316054\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85481162568843\n",
      " diff : 1.1072527883015937e-05\n",
      "at count 312 -> [0.02276866050352486, -5.091453572027961e-06, -0.0072530531912216875, 0.00011009348020224157, 2.6198827213936065e-07, 1.53464595381753e-07] and 1.054876006732156e-05\n",
      "loss is 100.8547898824252\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85480055316054\n",
      " diff : 1.0670735349549432e-05\n",
      "at count 313 -> [0.022768837305320613, -5.165578587987945e-06, -0.007253288002199025, 0.000110226257328599, 2.6248913545223576e-07, 1.5338484510441718e-07] and 1.0548839411967916e-05\n",
      "loss is 100.85477959509487\n",
      " loss is greater than learning limiter\n",
      " prev is 100.8547898824252\n",
      " diff : 1.0287330326264055e-05\n",
      "at count 314 -> [0.022769010007422742, -5.239706615574516e-06, -0.007253517373270615, 0.00011035901603631212, 2.6298995635661935e-07, 1.5330502329884692e-07] and 1.0548916767145593e-05\n",
      "loss is 100.85476967362338\n",
      " loss is greater than learning limiter\n",
      " prev is 100.85477959509487\n",
      " diff : 9.921471487928102e-06\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10.042647543034823"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Linear_Regression(y,X,learning_rate=0.00000001,learning_limiter= 50)\n",
    "model.train()\n",
    "model.rsme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.022769178704732784,\n",
       " -5.313837585016743e-06,\n",
       " -0.0072537414303618175,\n",
       " 0.00011049175675141501,\n",
       " 2.6349073583404174e-07,\n",
       " 1.5322513162088475e-07]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0548992178907699e-05"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.057593528843654"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X.iloc[5].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X1 transaction date                       2012.667\n",
       "X2 house age                                     0\n",
       "X3 distance to the nearest MRT station     2175.03\n",
       "X4 number of convenience stores                  3\n",
       "X5 latitude                               -0.00598\n",
       "X6 longitude                             -0.020821\n",
       "Y house price of unit area                    32.1\n",
       "Name: 5, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[5]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
